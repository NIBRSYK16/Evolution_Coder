"""
å®Œæ•´çš„ Qwen2.5-Coder æ¨¡å‹å¾®è°ƒä¸è¯„ä¼°ç³»ç»Ÿ
æ•´åˆæ¨¡å‹åŠ è½½ã€å¾®è°ƒã€è¯„ä¼°ä¸‰å¤§åŠŸèƒ½
æ–°å¢ï¼šç›´æ¥å¤§æ¨¡å‹é—®ç­”åŠŸèƒ½
"""

import gradio as gr
import torch
import json
import os
import sys
import re
import time
import threading
import subprocess
import tempfile
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional


# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ====== å…¨å±€å˜é‡ ======
# æ¨¡å‹ç›¸å…³
model = None
tokenizer = None
device = None

# çŠ¶æ€æ ‡å¿—
is_training = False
is_evaluating = False
is_generating = False
training_thread = None
evaluation_thread = None

# ç»“æœå­˜å‚¨
comparison_results = {}

# ====== APIé…ç½®ï¼ˆä»generate_mbpp_dataset.pyå¤åˆ¶ï¼‰ ======
API_CONFIG = {
    "qwen_32b_api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
    "qwen_14b_api_url": "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions",
    "api_key": "sk-1d1d9ecf1f1b446588871b3e6d5d3a30",
}

# ====== é»˜è®¤é…ç½® ======
DEFAULT_CONFIG = {
    # æ¨¡å‹é…ç½®
    "model_path": "./models/Qwen2.5-Coder-0.5B-Instruct",
    "finetuned_model_path": "./qwen2.5-coder-0.5b-finetuned",
    "human_eval_path": "./datasets/human-eval-v2-20210705.jsonl",
    
    # è®­ç»ƒé…ç½®
    "mbpp_dataset_path": "./datasets/mbpp_text_only.jsonl",  # MBPPåŸå§‹æ•°æ®é›†è·¯å¾„
    "training_dataset_path": "./mbpp_training_data/mbpp_training_dataset.jsonl",  # å¤„ç†åè®­ç»ƒé›†è·¯å¾„
    "output_dir": "./qwen2.5-coder-0.5b-finetuned",
    "num_epochs": 3,
    "learning_rate": 2e-4,
    "batch_size": 4,
    "use_lora": True,
    "use_4bit": False,
    
    # æ•°æ®ç”Ÿæˆé…ç½®
    "max_generate_items": 50,  # æœ€å¤§ç”Ÿæˆæ•°æ®é‡
    "generate_batch_size": 2,  # ç”Ÿæˆæ‰¹å¤§å°
    "max_retries": 3,  # APIé‡è¯•æ¬¡æ•°
    
    # è¯„ä¼°é…ç½®
    "max_tasks": 20,
    "max_tokens": 512,
    "temperature": 0.7,
    "top_p": 0.9,
    
    # é—®ç­”é…ç½®
    "max_new_tokens": 512,
    "gen_temperature": 0.8,
    "gen_top_p": 0.95
}

# ====== æ—¥å¿—æ”¶é›†å™¨ ======
class LogCollector:
    """æ”¶é›†æ‰€æœ‰æ—¥å¿—"""
    def __init__(self):
        self.logs = []
        self.lock = threading.Lock()
        
    def add_log(self, message):
        with self.lock:
            timestamp = datetime.now().strftime("%H:%M:%S")
            self.logs.append(f"[{timestamp}] {message}")
            
    def get_logs(self, last_n=100):
        with self.lock:
            return "\n".join(self.logs[-last_n:])
            
    def clear(self):
        with self.lock:
            self.logs.clear()

log_collector = LogCollector()

def log(message):
    """è®°å½•æ—¥å¿—"""
    log_collector.add_log(message)
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")

# ====== ä»generate_mbpp_dataset.pyå¤åˆ¶çš„å‡½æ•° ======
def call_qwen_api(api_url: str, prompt: str, model_name: str = "qwen2.5-coder-32b-instruct", 
                  max_tokens: int = 1024, temperature: float = 0.7, 
                  retries: int = 3) -> Tuple[bool, str]:
    """
    è°ƒç”¨Qwen APIç”Ÿæˆä»£ç 
    """
    # å»¶è¿Ÿå¯¼å…¥requests
    try:
        import requests
    except ImportError:
        log("âŒ æœªå®‰è£…requestsåº“ï¼Œæ— æ³•è°ƒç”¨API")
        return False, "è¯·å®‰è£…requestsåº“: pip install requests"
    
    headers = {
        "Authorization": f"Bearer {API_CONFIG['api_key']}",
        "Content-Type": "application/json"
    }
    
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œè¯·ç”Ÿæˆé«˜è´¨é‡ã€å¯è¿è¡Œçš„Pythonä»£ç ã€‚"},
        {"role": "user", "content": prompt}
    ]
    
    payload = {
        "model": model_name,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9
    }
    
    for attempt in range(retries):
        try:
            response = requests.post(api_url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            result = response.json()
            generated_code = result["choices"][0]["message"]["content"]
            
            # æå–ä»£ç å—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            code_pattern = r"```(?:python)?\n?(.*?)```"
            matches = re.findall(code_pattern, generated_code, re.DOTALL)
            
            if matches:
                generated_code = matches[0].strip()
            
            return True, generated_code
        except requests.exceptions.RequestException as e:
            if attempt == retries - 1:
                return False, f"APIè°ƒç”¨å¤±è´¥ï¼ˆå°è¯•{retries}æ¬¡ï¼‰: {str(e)}"
            time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
        except Exception as e:
            return False, f"APIå¤„ç†å¤±è´¥: {str(e)}"
    
    return False, "æœªçŸ¥é”™è¯¯"

def validate_code_with_14b(instruct: str, code: str) -> Tuple[bool, str]:
    """
    ä½¿ç”¨14Bæ¨¡å‹éªŒè¯ä»£ç æ˜¯å¦ç¬¦åˆæŒ‡ä»¤é€»è¾‘
    """
    validation_prompt = f"""
    è¯·åˆ†æä»¥ä¸‹ä»£ç æ˜¯å¦ç¬¦åˆç”¨æˆ·æŒ‡ä»¤çš„é€»è¾‘è¦æ±‚ï¼š
    
    ç”¨æˆ·æŒ‡ä»¤ï¼š{instruct}
    
    ç”Ÿæˆçš„ä»£ç ï¼š
    ```python
    {code}
    ```
    
    è¯·ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œåˆ¤æ–­ï¼š
    1. ä»£ç æ˜¯å¦å®Œæ•´å®ç°äº†æŒ‡ä»¤è¦æ±‚çš„åŠŸèƒ½
    2. ä»£ç é€»è¾‘æ˜¯å¦æ­£ç¡®
    3. æ˜¯å¦æœ‰æ˜æ˜¾çš„é€»è¾‘é”™è¯¯æˆ–ç¼ºå¤±
    
    è¯·ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
    [æ˜¯å¦é€šè¿‡]ï¼šæ˜¯/å¦
    [ç†ç”±]ï¼šç®€è¦è¯´æ˜ç†ç”±
    """
    
    success, response = call_qwen_api(
        API_CONFIG["qwen_14b_api_url"], 
        validation_prompt, 
        model_name="qwen2.5-coder-14b-instruct",
        max_tokens=256,
        temperature=0.3
    )
    
    if not success:
        return False, response
    
    # è§£æå“åº”
    if "[æ˜¯å¦é€šè¿‡]ï¼šæ˜¯" in response or ("é€šè¿‡" in response and "å¦" not in response):
        return True, response
    else:
        return False, response

def check_code_syntax(code: str) -> Tuple[bool, str]:
    """
    æ£€æŸ¥Pythonä»£ç çš„è¯­æ³•é”™è¯¯
    """
    try:
        # æ·»åŠ å¿…è¦çš„å¯¼å…¥
        full_code = "import math\nimport re\nimport heapq\nimport numpy as np\nimport collections\n" + code
        
        # å°è¯•ç¼–è¯‘
        compile(full_code, '<string>', 'exec')
        return True, "è¯­æ³•æ£€æŸ¥é€šè¿‡"
    except SyntaxError as e:
        return False, f"è¯­æ³•é”™è¯¯: {str(e)}"
    except Exception as e:
        return False, f"ä»£ç æ£€æŸ¥é”™è¯¯: {str(e)}"

def extract_function_name(code: str) -> str:
    """
    ä»ä»£ç ä¸­æå–å‡½æ•°å
    """
    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå‡½æ•°å®šä¹‰
    pattern = r'def\s+(\w+)\s*\('
    match = re.search(pattern, code)
    if match:
        return match.group(1)
    return "unknown_function"

def run_basic_test(code: str, function_name: str) -> Tuple[bool, str]:
    """
    è¿è¡ŒåŸºæœ¬æµ‹è¯•ï¼šæ£€æŸ¥å‡½æ•°æ˜¯å¦å¯ä»¥æ­£å¸¸è°ƒç”¨
    """
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as f:
            # æ·»åŠ å¿…è¦çš„å¯¼å…¥
            f.write("import math\nimport re\nimport heapq\nimport numpy as np\nimport collections\n")
            f.write(code)
            f.write(f"\n\n# åŸºæœ¬æµ‹è¯•\nif __name__ == '__main__':\n")
            f.write(f"    try:\n")
            f.write(f"        # æ£€æŸ¥å‡½æ•°æ˜¯å¦å­˜åœ¨\n")
            f.write(f"        if '{function_name}' in dir():\n")
            f.write(f"            func = {function_name}\n")
            f.write(f"            print('å‡½æ•°å­˜åœ¨ï¼Œå¯ä»¥è°ƒç”¨')\n")
            f.write(f"        else:\n")
            f.write(f"            print('å‡½æ•°ä¸å­˜åœ¨')\n")
            f.write(f"    except Exception as e:\n")
            f.write(f"        print(f'æµ‹è¯•å¤±è´¥: {{e}}')\n")
            temp_file = f.name
        
        result = subprocess.run(
            ['python', temp_file],
            capture_output=True,
            text=True,
            timeout=5
        )
        
        os.unlink(temp_file)
        
        if result.returncode == 0 and "å‡½æ•°å­˜åœ¨" in result.stdout:
            return True, "åŸºæœ¬æµ‹è¯•é€šè¿‡"
        else:
            return False, f"åŸºæœ¬æµ‹è¯•å¤±è´¥: {result.stderr or result.stdout}"
            
    except Exception as e:
        if os.path.exists(temp_file):
            os.unlink(temp_file)
        return False, f"æµ‹è¯•æ‰§è¡Œé”™è¯¯: {str(e)}"

def process_single_instruction(instruction: str, index: int) -> Tuple[bool, str, str]:
    """
    å¤„ç†å•ä¸ªæŒ‡ä»¤ï¼Œç”Ÿæˆä»£ç å¹¶éªŒè¯
    è¿”å›: (æ˜¯å¦æˆåŠŸ, ç”Ÿæˆçš„ä»£ç , éªŒè¯ç»“æœ)
    """
    log(f"[{index}] å¤„ç†æŒ‡ä»¤: {instruction[:80]}...")
    
    # æ­¥éª¤1: ä½¿ç”¨32Bæ¨¡å‹ç”Ÿæˆä»£ç 
    log(f"[{index}] è°ƒç”¨32B APIç”Ÿæˆä»£ç ...")
    success, code = call_qwen_api(
        API_CONFIG["qwen_32b_api_url"],
        instruction,
        model_name="qwen2.5-coder-32b-instruct"
    )
    
    if not success:
        log(f"[{index}] âŒ ä»£ç ç”Ÿæˆå¤±è´¥: {code}")
        return False, "", f"ä»£ç ç”Ÿæˆå¤±è´¥: {code}"
    
    # æ˜¾ç¤ºç”Ÿæˆçš„ä»£ç é¢„è§ˆ
    code_lines = code.split('\n')
    preview_lines = min(5, len(code_lines))
    code_preview = '\n'.join(code_lines[:preview_lines])
    log(f"[{index}] âœ… ä»£ç ç”ŸæˆæˆåŠŸ")
    log(f"[{index}] ä»£ç é¢„è§ˆï¼ˆå‰{preview_lines}è¡Œï¼‰:\n{code_preview}")
    log(f"[{index}] ä»£ç æ€»é•¿åº¦: {len(code)} å­—ç¬¦, {len(code_lines)} è¡Œ")
    
    # æ­¥éª¤2: è¯­æ³•æ£€æŸ¥
    log(f"[{index}] è¿›è¡Œè¯­æ³•æ£€æŸ¥...")
    syntax_ok, syntax_msg = check_code_syntax(code)
    if not syntax_ok:
        log(f"[{index}] âŒ {syntax_msg}")
        return False, "", syntax_msg
    
    log(f"[{index}] âœ… è¯­æ³•æ£€æŸ¥é€šè¿‡")
    
    # æ­¥éª¤3: é€»è¾‘éªŒè¯ï¼ˆ14Bæ¨¡å‹ï¼‰
    log(f"[{index}] è¿›è¡Œé€»è¾‘éªŒè¯ï¼ˆ14Bæ¨¡å‹ï¼‰...")
    logic_ok, logic_msg = validate_code_with_14b(instruction, code)
    
    if not logic_ok:
        log(f"[{index}] âŒ é€»è¾‘éªŒè¯å¤±è´¥: {logic_msg[:100]}")
        return False, "", f"é€»è¾‘éªŒè¯å¤±è´¥: {logic_msg[:100]}"
    
    log(f"[{index}] âœ… é€»è¾‘éªŒè¯é€šè¿‡")
    
    # æ­¥éª¤4: åŸºæœ¬æµ‹è¯•
    log(f"[{index}] è¿›è¡ŒåŸºæœ¬æµ‹è¯•...")
    function_name = extract_function_name(code)
    test_ok, test_msg = run_basic_test(code, function_name)
    
    if not test_ok:
        log(f"[{index}] âš ï¸ {test_msg} (ä½†ä»ä¿å­˜)")
        # åŸºæœ¬æµ‹è¯•å¤±è´¥ä¸ä¸€å®šæ„å‘³ç€ä»£ç æœ‰é—®é¢˜ï¼Œç»§ç»­å¤„ç†
    else:
        log(f"[{index}] âœ… åŸºæœ¬æµ‹è¯•é€šè¿‡")
    
    log(f"[{index}] âœ… å¤„ç†å®Œæˆï¼Œæ•°æ®å¯¹åˆæ ¼")
    
    return True, code, "éªŒè¯é€šè¿‡"

def generate_mbpp_training_data(mbpp_path: str, output_path: str, max_items: int = 50, 
                              start_index: int = 0) -> Tuple[bool, str]:
    """
    ç”ŸæˆMBPPè®­ç»ƒæ•°æ®
    """
    try:
        # å¯¼å…¥requestsï¼ˆå»¶è¿Ÿå¯¼å…¥ä»¥é¿å…ä¾èµ–é—®é¢˜ï¼‰
        try:
            import requests
        except ImportError:
            log("âŒ æœªå®‰è£…requestsåº“ï¼Œæ— æ³•è°ƒç”¨API")
            return False, "è¯·å®‰è£…requestsåº“: pip install requests"
        
        log(f"è¯»å–MBPPæ•°æ®é›†: {mbpp_path}")
        if not os.path.exists(mbpp_path):
            return False, f"MBPPæ•°æ®é›†ä¸å­˜åœ¨: {mbpp_path}"
        
        instructions = []
        with open(mbpp_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and line.startswith('"') and line.endswith('"'):
                    # ç§»é™¤å¼•å·
                    instruction = line[1:-1]
                    instructions.append(instruction)
        
        total_instructions = len(instructions)
        log(f"å…±è¯»å– {total_instructions} æ¡æŒ‡ä»¤")
        
        # é™åˆ¶å¤„ç†æ•°é‡
        if max_items:
            instructions = instructions[:max_items]
            total_instructions = len(instructions)
            log(f"é™åˆ¶å¤„ç†æ•°é‡ä¸º: {total_instructions}")
        
        # è·³è¿‡å·²å¤„ç†çš„
        if start_index > 0:
            instructions = instructions[start_index:]
            log(f"ä»ç´¢å¼• {start_index} å¼€å§‹å¤„ç†")
        
        if not instructions:
            return True, "æ²¡æœ‰éœ€è¦å¤„ç†çš„æŒ‡ä»¤"
        
        log(f"å¼€å§‹å¤„ç† {len(instructions)} æ¡æŒ‡ä»¤...")
        
        # å¤„ç†æ¯æ¡æŒ‡ä»¤
        successful_pairs = []
        
        for i, instruction in enumerate(instructions, start=1):
            try:
                success, code, validation_msg = process_single_instruction(instruction, i)
                
                if success:
                    training_pair = {
                        "instruction": instruction,
                        "code": code,
                        "metadata": {
                            "index": i,
                            "timestamp": datetime.now().isoformat(),
                            "validation_result": validation_msg,
                            "source": "mbpp_dataset_generated"
                        }
                    }
                    successful_pairs.append(training_pair)
                    log(f"[{i}] âœ… æˆåŠŸç”Ÿæˆæ•°æ®å¯¹")
                else:
                    log(f"[{i}] âŒ æ•°æ®å¯¹ç”Ÿæˆå¤±è´¥: {validation_msg}")
                
                # é¿å…APIè°ƒç”¨è¿‡äºé¢‘ç¹
                time.sleep(0.5)
                
            except Exception as e:
                log(f"[{i}] âŒ å¤„ç†å¼‚å¸¸: {str(e)}")
        
        # ä¿å­˜è®­ç»ƒæ•°æ®
        if successful_pairs:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                for pair in successful_pairs:
                    f.write(json.dumps({
                        "instruction": pair["instruction"],
                        "code": pair["code"]
                    }, ensure_ascii=False) + '\n')
            
            log(f"âœ… è®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ: {len(successful_pairs)}/{len(instructions)} æ¡æˆåŠŸ")
            log(f"è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
            
            return True, f"æˆåŠŸç”Ÿæˆ {len(successful_pairs)} ä¸ªè®­ç»ƒæ•°æ®å¯¹"
        else:
            return False, "æœªç”Ÿæˆä»»ä½•è®­ç»ƒæ•°æ®å¯¹"
            
    except Exception as e:
        return False, f"ç”Ÿæˆè®­ç»ƒæ•°æ®æ—¶å‡ºé”™: {str(e)}"

# ====== æ¨¡å‹é—®ç­”åŠŸèƒ½æ¨¡å— ======
def generate_code_with_local_model(instruction: str, config: Dict) -> Tuple[str, str]:
    """
    ä½¿ç”¨æœ¬åœ°åŠ è½½çš„æ¨¡å‹ç”Ÿæˆä»£ç 
    """
    global model, tokenizer, device, is_generating
    
    if model is None or tokenizer is None:
        return "âŒ é”™è¯¯: æ¨¡å‹æœªåŠ è½½", "è¯·å…ˆåŠ è½½æ¨¡å‹"
    
    if is_generating:
        return "âš ï¸ æ­£åœ¨ç”Ÿæˆä¸­ï¼Œè¯·ç¨å€™...", ""
    
    is_generating = True
    
    try:
        log(f"å¼€å§‹ç”Ÿæˆä»£ç ï¼ŒæŒ‡ä»¤: {instruction[:100]}...")
        
        # å‡†å¤‡è¾“å…¥
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonç¼–ç¨‹åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·æŒ‡ä»¤ç”Ÿæˆæ­£ç¡®ã€é«˜æ•ˆçš„Pythonä»£ç ã€‚"},
            {"role": "user", "content": instruction}
        ]
        
        # ä½¿ç”¨Qwençš„èŠå¤©æ¨¡æ¿
        try:
            # å°è¯•ä½¿ç”¨tokenizerçš„apply_chat_template
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
        except:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰‹åŠ¨æ„å»ºQwenæ ¼å¼
            text = f"<|im_start|>system\n{messages[0]['content']}<|im_end|>\n"
            text += f"<|im_start|>user\n{messages[1]['content']}<|im_end|>\n"
            text += f"<|im_start|>assistant\n"
        
        # ç¼–ç è¾“å…¥
        inputs = tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=1024
        )
        
        # ç§»åˆ°è®¾å¤‡
        if device == "cuda":
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ç”Ÿæˆä»£ç 
        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=config.get("max_new_tokens", DEFAULT_CONFIG["max_new_tokens"]),
                temperature=config.get("gen_temperature", DEFAULT_CONFIG["gen_temperature"]),
                top_p=config.get("gen_top_p", DEFAULT_CONFIG["gen_top_p"]),
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                num_beams=1,
                repetition_penalty=1.1
            )
        
        # è§£ç ç”Ÿæˆçš„ä»£ç 
        generated_tokens = generated_ids[0][inputs['input_ids'].shape[1]:]
        generated_code = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        # æ¸…ç†ç‰¹æ®Šæ ‡è®°
        generated_code = generated_code.replace("<|im_end|>", "").replace("<|im_start|>", "").strip()
        
        # æå–å¯èƒ½çš„ä»£ç å—
        code_pattern = r"```(?:python)?\n?(.*?)```"
        matches = re.findall(code_pattern, generated_code, re.DOTALL)
        
        if matches:
            generated_code = matches[0].strip()
        
        # æ¸…ç†ä»£ç ä¸­çš„å¤šä½™è¯´æ˜
        lines = generated_code.split('\n')
        cleaned_lines = []
        in_code_block = False
        for line in lines:
            if line.strip().startswith('def ') or line.strip().startswith('class ') or line.strip().startswith('import ') or line.strip().startswith('from '):
                in_code_block = True
            if in_code_block or line.strip().startswith('#') or line.strip().startswith('"""') or line.strip().startswith("'''"):
                cleaned_lines.append(line)
        
        generated_code = '\n'.join(cleaned_lines)
        
        log(f"âœ… ä»£ç ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(generated_code)} å­—ç¬¦")
        
        return "âœ… ä»£ç ç”ŸæˆæˆåŠŸ", generated_code
        
    except Exception as e:
        error_msg = f"âŒ ç”Ÿæˆä»£ç æ—¶å‡ºé”™: {str(e)}"
        log(error_msg)
        return error_msg, ""
        
    finally:
        is_generating = False

def save_instruction_to_mbpp(instruction: str, mbpp_path: str = None):
    """
    å°†æŒ‡ä»¤ä¿å­˜åˆ°MBPPæ•°æ®é›†ï¼ˆæ·»åŠ å¼•å·ç¡®ä¿æ ¼å¼ç»Ÿä¸€ï¼‰
    """
    try:
        if mbpp_path is None:
            mbpp_path = DEFAULT_CONFIG["mbpp_dataset_path"]
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(mbpp_path), exist_ok=True)
        
        # æ¸…ç†æŒ‡ä»¤ï¼šç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
        cleaned_instruction = instruction.strip()
        
        # ç¡®ä¿æŒ‡ä»¤ç”¨åŒå¼•å·åŒ…è£¹
        if not (cleaned_instruction.startswith('"') and cleaned_instruction.endswith('"')):
            # è½¬ä¹‰å†…éƒ¨çš„åŒå¼•å·
            cleaned_instruction = cleaned_instruction.replace('"', '\\"')
            cleaned_instruction = f'"{cleaned_instruction}"'
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(mbpp_path, 'a', encoding='utf-8') as f:
            f.write(cleaned_instruction + '\n')
        
        log(f"âœ… æŒ‡ä»¤å·²ä¿å­˜åˆ°MBPPæ•°æ®é›†: {cleaned_instruction[:100]}...")
        return True, f"æŒ‡ä»¤å·²ä¿å­˜åˆ° {mbpp_path}"
        
    except Exception as e:
        error_msg = f"âŒ ä¿å­˜æŒ‡ä»¤å¤±è´¥: {str(e)}"
        log(error_msg)
        return False, error_msg

def process_instruction_with_local_model(instruction: str, temperature: float, top_p: float, 
                                        max_new_tokens: int, mbpp_path: str = None) -> Tuple[str, str, str]:
    """
    å¤„ç†ç”¨æˆ·æŒ‡ä»¤ï¼šå¦‚æœæ˜¯"è‡ªæˆ‘æ¼”åŒ–"åˆ™å¼€å§‹å¾®è°ƒï¼Œå¦åˆ™ç”Ÿæˆä»£ç å¹¶ä¿å­˜æŒ‡ä»¤
    """
    global is_training, training_thread
    
    # æ¸…ç†æŒ‡ä»¤
    instruction = instruction.strip()
    
    # æ£€æŸ¥æ˜¯å¦ä¸º"è‡ªæˆ‘æ¼”åŒ–"æŒ‡ä»¤
    if instruction.lower() == "è‡ªæˆ‘æ¼”åŒ–":
        log("æ£€æµ‹åˆ°'è‡ªæˆ‘æ¼”åŒ–'æŒ‡ä»¤ï¼Œå¼€å§‹å¾®è°ƒæµç¨‹...")
        
        # æ£€æŸ¥MBPPæ•°æ®é›†æ˜¯å¦å­˜åœ¨
        if mbpp_path is None:
            mbpp_path = DEFAULT_CONFIG["mbpp_dataset_path"]
        
        if not os.path.exists(mbpp_path):
            error_msg = f"âŒ MBPPæ•°æ®é›†ä¸å­˜åœ¨: {mbpp_path}"
            log(error_msg)
            return error_msg, "", ""
        
        # æ£€æŸ¥æ•°æ®é›†å¤§å°
        try:
            with open(mbpp_path, 'r', encoding='utf-8') as f:
                lines = sum(1 for _ in f)
        except:
            lines = 0
        
        if lines == 0:
            error_msg = f"âŒ MBPPæ•°æ®é›†ä¸ºç©º: {mbpp_path}"
            log(error_msg)
            return error_msg, "", ""
        
        log(f"MBPPæ•°æ®é›†åŒ…å« {lines} æ¡æŒ‡ä»¤")
        
        # å¼€å§‹å¾®è°ƒ
        if is_training:
            return "âš ï¸ è®­ç»ƒå·²ç»åœ¨è¿›è¡Œä¸­...", "", ""
        
        # å‡†å¤‡è®­ç»ƒé…ç½®
        train_config = {
            "model_path": DEFAULT_CONFIG["model_path"],
            "mbpp_dataset_path": mbpp_path,
            "output_dir": DEFAULT_CONFIG["output_dir"],
            "num_epochs": DEFAULT_CONFIG["num_epochs"],
            "learning_rate": DEFAULT_CONFIG["learning_rate"],
            "batch_size": DEFAULT_CONFIG["batch_size"],
            "max_generate_items": min(50, lines),  # é™åˆ¶ç”Ÿæˆæ•°é‡
            "use_lora": DEFAULT_CONFIG["use_lora"],
            "use_4bit": DEFAULT_CONFIG["use_4bit"]
        }
        
        # å¼€å§‹è®­ç»ƒçº¿ç¨‹
        training_thread = TrainingThread(train_config, log)
        is_training = True
        training_thread.start()
        
        status_msg = f"""
ğŸš€ å¼€å§‹è‡ªæˆ‘æ¼”åŒ–ï¼ˆå¾®è°ƒï¼‰...
ä½¿ç”¨æŒ‡ä»¤: {lines} æ¡
è¾“å‡ºç›®å½•: {DEFAULT_CONFIG['output_dir']}
è®­ç»ƒè½®æ•°: {DEFAULT_CONFIG['num_epochs']}
å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}
        """
        
        log(status_msg)
        return status_msg, "", ""
    
    else:
        # æ­£å¸¸ç”Ÿæˆä»£ç æµç¨‹
        log(f"å¤„ç†ç”¨æˆ·æŒ‡ä»¤: {instruction[:100]}...")
        
        # æ„å»ºé…ç½®å­—å…¸
        config = {
            "max_new_tokens": max_new_tokens,
            "gen_temperature": temperature,
            "gen_top_p": top_p
        }
        
        # ç”Ÿæˆä»£ç 
        status, code = generate_code_with_local_model(instruction, config)
        
        # ä¿å­˜æŒ‡ä»¤åˆ°MBPPæ•°æ®é›†ï¼ˆå¸¦å¼•å·ï¼‰
        save_success, save_msg = save_instruction_to_mbpp(instruction, mbpp_path)
        
        if save_success:
            save_status = f"âœ… æŒ‡ä»¤å·²ä¿å­˜åˆ°MBPPæ•°æ®é›†"
        else:
            save_status = f"âš ï¸ ä¿å­˜æŒ‡ä»¤å¤±è´¥: {save_msg}"
        
        return status, code, save_status

# ====== æ¨¡å‹åŠ è½½æ¨¡å— ======
def load_model_interface(model_path):
    """åŠ è½½æ¨¡å‹ç•Œé¢å‡½æ•°"""
    global model, tokenizer, device
    
    if not model_path or model_path.strip() == "":
        model_path = DEFAULT_CONFIG["model_path"]
    
    if not os.path.exists(model_path):
        return f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}", False
    
    try:
        log("ğŸš€ å¼€å§‹åŠ è½½æ¨¡å‹...")
        
        # åŠ¨æ€å¯¼å…¥
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            local_files_only=True,
            trust_remote_code=True
        )
        
        # ç¡®å®šè®¾å¤‡
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            local_files_only=True,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True
        )
        
        model.eval()
        
        info = f"""
âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼
æ¨¡å‹è·¯å¾„: {model_path}
ä½¿ç”¨è®¾å¤‡: {device}
æ¨¡å‹å‚æ•°é‡: çº¦0.5B
Tokenizer: å·²åŠ è½½
        """
        
        log(info)
        return "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ", True
        
    except Exception as e:
        error_msg = f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}"
        log(error_msg)
        return error_msg, False

# ====== æ¨¡å‹è®­ç»ƒæ¨¡å—ï¼ˆä¿®æ”¹ç‰ˆï¼Œä½¿ç”¨MBPPæ•°æ®ç”Ÿæˆï¼‰ ======
class TrainingThread(threading.Thread):
    """è®­ç»ƒçº¿ç¨‹"""
    def __init__(self, config, callback=None):
        super().__init__()
        self.config = config
        self.callback = callback
        self.daemon = True
        
    def log(self, message):
        if self.callback:
            self.callback(message)
        log(message)
        
    def run(self):
        try:
            # æ­¥éª¤1: ç”Ÿæˆè®­ç»ƒæ•°æ®
            self.log("=" * 60)
            self.log("ç¬¬ä¸€æ­¥: ç”Ÿæˆè®­ç»ƒæ•°æ®")
            self.log("=" * 60)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆè®­ç»ƒæ•°æ®
            mbpp_path = self.config.get('mbpp_dataset_path', DEFAULT_CONFIG["mbpp_dataset_path"])
            training_data_path = self.config.get('training_dataset_path', DEFAULT_CONFIG["training_dataset_path"])
            max_generate_items = self.config.get('max_generate_items', DEFAULT_CONFIG["max_generate_items"])
            
            # å¦‚æœè®­ç»ƒæ•°æ®ä¸å­˜åœ¨æˆ–éœ€è¦é‡æ–°ç”Ÿæˆ
            if not os.path.exists(training_data_path):
                self.log(f"è®­ç»ƒæ•°æ®ä¸å­˜åœ¨ï¼Œå¼€å§‹ç”Ÿæˆ...")
                self.log(f"MBPPæ•°æ®é›†: {mbpp_path}")
                self.log(f"è¾“å‡ºè·¯å¾„: {training_data_path}")
                self.log(f"æœ€å¤§ç”Ÿæˆæ•°é‡: {max_generate_items}")
                
                success, msg = generate_mbpp_training_data(
                    mbpp_path, 
                    training_data_path,
                    max_items=max_generate_items
                )
                
                if not success:
                    self.log(f"âŒ ç”Ÿæˆè®­ç»ƒæ•°æ®å¤±è´¥: {msg}")
                    return
                
                self.log(f"âœ… {msg}")
            else:
                # æ£€æŸ¥ç°æœ‰è®­ç»ƒæ•°æ®
                with open(training_data_path, 'r', encoding='utf-8') as f:
                    lines = sum(1 for _ in f)
                self.log(f"âœ… ä½¿ç”¨ç°æœ‰è®­ç»ƒæ•°æ®: {training_data_path}")
                self.log(f"ç°æœ‰è®­ç»ƒæ ·æœ¬æ•°: {lines}")
            
            # æ­¥éª¤2: åŠ è½½æ¨¡å‹è¿›è¡Œå¾®è°ƒ
            self.log("=" * 60)
            self.log("ç¬¬äºŒæ­¥: å¼€å§‹æ¨¡å‹å¾®è°ƒ")
            self.log("=" * 60)
            
            self.log("å¼€å§‹å¯¼å…¥è®­ç»ƒåº“...")
            
            # åŠ¨æ€å¯¼å…¥è®­ç»ƒæ‰€éœ€çš„åº“
            from transformers import (
                AutoTokenizer,
                AutoModelForCausalLM,
                TrainingArguments,
                Trainer,
                DataCollatorForLanguageModeling,
                BitsAndBytesConfig
            )
            
            from datasets import Dataset
            import warnings
            warnings.filterwarnings("ignore")
            
            self.log("åº“å¯¼å…¥å®Œæˆ")
            
            # åŠ è½½æ¨¡å‹
            self.log(f"åŠ è½½æ¨¡å‹: {self.config['model_path']}")
            global model, tokenizer, device
            
            # åŠ è½½tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                self.config['model_path'],
                trust_remote_code=True,
                padding_side="right",
                use_fast=True
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                
            # ç¡®å®šè®¾å¤‡
            device = "cuda" if torch.cuda.is_available() else "cpu"
            self.log(f"ä½¿ç”¨è®¾å¤‡: {device}")
            
            # åŠ è½½æ¨¡å‹
            model = AutoModelForCausalLM.from_pretrained(
                self.config['model_path'],
                local_files_only=True,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                low_cpu_mem_usage=True
            )
            
            self.log("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
            
            # åŠ è½½æ•°æ®é›†
            self.log(f"åŠ è½½è®­ç»ƒæ•°æ®é›†: {training_data_path}")
            data = []
            try:
                with open(training_data_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            data.append(json.loads(line))
            except Exception as e:
                self.log(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {str(e)}")
                return
                
            self.log(f"æ•°æ®é›†å¤§å°: {len(data)} ä¸ªæ ·æœ¬")
            
            if len(data) == 0:
                self.log("âŒ æ•°æ®é›†ä¸ºç©º")
                return
                
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            self.log("å‡†å¤‡è®­ç»ƒæ•°æ®...")
            processed_data = []
            for item in data[:100]:  # é™åˆ¶æ ·æœ¬æ•°é‡ï¼Œé¿å…å†…å­˜ä¸è¶³
                instruction = item.get("instruction", "")
                code = item.get("code", "")
                
                # åˆ›å»ºæ¨¡å‹è¾“å…¥æ ¼å¼
                messages = [
                    {"role": "system", "content": "You are a helpful AI assistant that writes Python code."},
                    {"role": "user", "content": instruction},
                    {"role": "assistant", "content": code}
                ]
                
                # ä½¿ç”¨Qwenç‰¹å®šçš„æ ¼å¼
                text = f"<|im_start|>system\n{messages[0]['content']}<|im_end|>\n"
                text += f"<|im_start|>user\n{messages[1]['content']}<|im_end|>\n"
                text += f"<|im_start|>assistant\n{messages[2]['content']}<|im_end|>\n"
                
                processed_data.append({"text": text})
            
            # åˆ›å»ºæ•°æ®é›†
            dataset = Dataset.from_list(processed_data)
            
            # åˆ†å‰²è®­ç»ƒ/éªŒè¯é›†
            if len(dataset) > 20:
                split_dataset = dataset.train_test_split(test_size=0.1, seed=42)
                train_dataset = split_dataset["train"]
                eval_dataset = split_dataset["test"]
            else:
                train_dataset = dataset
                eval_dataset = dataset.select(range(min(5, len(dataset))))
                
            self.log(f"è®­ç»ƒé›†: {len(train_dataset)}, éªŒè¯é›†: {len(eval_dataset)}")
            
            # æ•°æ®é¢„å¤„ç†
            def preprocess_function(examples):
                return tokenizer(
                    examples["text"],
                    truncation=True,
                    max_length=512,  # å‡å°‘é•¿åº¦ä»¥èŠ‚çœå†…å­˜
                    padding="max_length",
                )
                
            self.log("é¢„å¤„ç†æ•°æ®...")
            tokenized_train = train_dataset.map(
                preprocess_function,
                batched=True,
                remove_columns=train_dataset.column_names,
            )
            tokenized_eval = eval_dataset.map(
                preprocess_function,
                batched=True,
                remove_columns=eval_dataset.column_names,
            )
            
            # æ•°æ®æ•´ç†å™¨
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=tokenizer,
                mlm=False,
            )
            
            # è®­ç»ƒå‚æ•°
            training_args = TrainingArguments(
                output_dir=self.config['output_dir'],
                num_train_epochs=self.config['num_epochs'],
                per_device_train_batch_size=self.config['batch_size'],
                per_device_eval_batch_size=self.config['batch_size'],
                gradient_accumulation_steps=2,
                warmup_steps=50,
                logging_steps=5,
                save_strategy="epoch",
                eval_strategy="epoch",
                learning_rate=self.config['learning_rate'],
                weight_decay=0.01,
                fp16=False, # torch.cuda.is_available(),
                push_to_hub=False,
                report_to="none",
                gradient_checkpointing=True,
            )
            
            # åˆ›å»ºTrainer
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=tokenized_train,
                eval_dataset=tokenized_eval,
                tokenizer=tokenizer,
                data_collator=data_collator,
            )
            
            # å¼€å§‹è®­ç»ƒ
            self.log("å¼€å§‹è®­ç»ƒ...")
            trainer.train()
            
            # ä¿å­˜æ¨¡å‹
            self.log("ä¿å­˜æ¨¡å‹...")
            trainer.save_model()
            tokenizer.save_pretrained(self.config['output_dir'])
            
            self.log("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
            self.log(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {self.config['output_dir']}")
            
        except ImportError as e:
            self.log(f"âŒ ç¼ºå°‘ä¾èµ–åº“: {str(e)}")
            self.log("è¯·è¿è¡Œ: pip install torch transformers datasets")
        except Exception as e:
            self.log(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            import traceback
            self.log(traceback.format_exc())
        finally:
            global is_training
            is_training = False

def start_training_interface(config_data):
    """å¼€å§‹è®­ç»ƒç•Œé¢å‡½æ•°"""
    global is_training, training_thread
    
    if is_training:
        return "âš ï¸ è®­ç»ƒå·²ç»åœ¨è¿›è¡Œä¸­...", False
    
    # æ›´æ–°é…ç½®
    config = DEFAULT_CONFIG.copy()
    config.update(config_data)
    
    # æ£€æŸ¥å¿…è¦å‚æ•°
    required_fields = ["model_path", "mbpp_dataset_path", "output_dir"]
    for field in required_fields:
        if not config.get(field):
            return f"âŒ è¯·å¡«å†™{field}", False
    
    # æ£€æŸ¥MBPPæ•°æ®é›†
    mbpp_path = config.get("mbpp_dataset_path", DEFAULT_CONFIG["mbpp_dataset_path"])
    if not os.path.exists(mbpp_path):
        return f"âŒ MBPPæ•°æ®é›†ä¸å­˜åœ¨: {mbpp_path}", False
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config["output_dir"], exist_ok=True)
    
    # å¼€å§‹è®­ç»ƒçº¿ç¨‹
    training_thread = TrainingThread(config, log)
    is_training = True
    training_thread.start()
    
    start_msg = f"""
ğŸš€ å¼€å§‹æ¨¡å‹å¾®è°ƒä»»åŠ¡...

ç¬¬ä¸€é˜¶æ®µ: ç”Ÿæˆè®­ç»ƒæ•°æ®
- MBPPæ•°æ®é›†: {config.get('mbpp_dataset_path', DEFAULT_CONFIG["mbpp_dataset_path"])}
- æœ€å¤§ç”Ÿæˆæ•°é‡: {config.get('max_generate_items', DEFAULT_CONFIG["max_generate_items"])}
- è¾“å‡ºè·¯å¾„: {config.get('training_dataset_path', DEFAULT_CONFIG["training_dataset_path"])}

ç¬¬äºŒé˜¶æ®µ: æ¨¡å‹å¾®è°ƒ
- æ¨¡å‹: {config['model_path']}
- è¾“å‡ºç›®å½•: {config['output_dir']}
- è®­ç»ƒè½®æ•°: {config['num_epochs']}
- å­¦ä¹ ç‡: {config['learning_rate']}
- æ‰¹å¤§å°: {config['batch_size']}

è®­ç»ƒæ—¥å¿—å°†åœ¨ä¸‹æ–¹æ˜¾ç¤º...
    """
    
    log(start_msg)
    return "âœ… è®­ç»ƒå·²å¼€å§‹", True

# ====== æ¨¡å‹è¯„ä¼°æ¨¡å— ======
class EvaluationThread(threading.Thread):
    """è¯„ä¼°çº¿ç¨‹"""
    def __init__(self, config, callback=None):
        super().__init__()
        self.config = config
        self.callback = callback
        self.daemon = True
        self.result = None
        
    def log(self, message):
        if self.callback:
            self.callback(message)
        log(message)
        
    def run(self):
        try:
            self.evaluate_models()
        except Exception as e:
            self.log(f"è¯„ä¼°çº¿ç¨‹å‡ºé”™: {str(e)}")
            import traceback
            self.log(traceback.format_exc())
        finally:
            global is_evaluating
            is_evaluating = False
            
    def evaluate_models(self):
        """è¯„ä¼°æ¨¡å‹"""
        self.log("å¼€å§‹å¯¼å…¥è¯„ä¼°åº“...")
        
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        original_path = self.config["model_path"]
        finetuned_path = self.config["finetuned_model_path"]
        dataset_path = self.config["human_eval_path"]
        
        if not os.path.exists(original_path):
            self.log(f"âŒ åŸå§‹æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {original_path}")
            return
            
        if not os.path.exists(finetuned_path):
            self.log(f"âŒ å¾®è°ƒæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {finetuned_path}")
            return
            
        if not os.path.exists(dataset_path):
            self.log(f"âŒ HumanEvalæ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
            self.log("è¯·ä» https://github.com/openai/human-eval ä¸‹è½½æ•°æ®é›†")
            return
        
        # è¯„ä¼°åŸå§‹æ¨¡å‹
        self.log("="*60)
        self.log("å¼€å§‹è¯„ä¼°åŸå§‹æ¨¡å‹...")
        original_result = self.evaluate_single_model(
            original_path, 
            "åŸå§‹æ¨¡å‹",
            base_model_path=None
        )
        
        if original_result:
            self.log(f"åŸå§‹æ¨¡å‹è¯„ä¼°å®Œæˆ: é€šè¿‡ç‡ {original_result['pass_rate']:.2f}%")
            
            # æ¸…ç†å†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
        # è¯„ä¼°å¾®è°ƒæ¨¡å‹
        self.log("="*60)
        self.log("å¼€å§‹è¯„ä¼°å¾®è°ƒåæ¨¡å‹...")
        finetuned_result = self.evaluate_single_model(
            finetuned_path,
            "å¾®è°ƒåæ¨¡å‹",
            base_model_path=original_path  # LoRAéœ€è¦åŸºç¡€æ¨¡å‹
        )
        
        if finetuned_result:
            self.log(f"å¾®è°ƒåæ¨¡å‹è¯„ä¼°å®Œæˆ: é€šè¿‡ç‡ {finetuned_result['pass_rate']:.2f}%")
            
        # å¯¹æ¯”ç»“æœ
        if original_result and finetuned_result:
            comparison = self.compare_results(original_result, finetuned_result)
            global comparison_results
            comparison_results = comparison
            
            self.log("="*60)
            self.log("æ¨¡å‹å¯¹æ¯”å®Œæˆï¼")
            self.log(f"åŸå§‹æ¨¡å‹é€šè¿‡ç‡: {original_result['pass_rate']:.2f}%")
            self.log(f"å¾®è°ƒåæ¨¡å‹é€šè¿‡ç‡: {finetuned_result['pass_rate']:.2f}%")
            self.log(f"æå‡: {comparison['improvement']:.2f}%")
            
            # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            result_file = f"./evaluation_results_{timestamp}.json"
            
            results = {
                "original": original_result,
                "finetuned": finetuned_result,
                "comparison": comparison,
                "timestamp": timestamp,
                "config": self.config
            }
            
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
                
            self.log(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
            
        self.log("ğŸ‰ æ¨¡å‹è¯„ä¼°å…¨éƒ¨å®Œæˆï¼")
        
    def evaluate_single_model(self, model_path, model_name, base_model_path=None):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        try:
            # åŠ è½½æ¨¡å‹
            self.log(f"åŠ è½½{model_name}: {model_path}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯LoRA adapter
            is_lora = os.path.exists(os.path.join(model_path, "adapter_config.json"))
            from transformers import AutoTokenizer, AutoModelForCausalLM            
            
            if is_lora and base_model_path:
                # ä½¿ç”¨LoRA adapter
                self.log("æ£€æµ‹åˆ°LoRA adapterï¼ŒåŠ è½½åŸºç¡€æ¨¡å‹å¹¶åˆå¹¶adapter")
                
                try:
                    from peft import PeftModel

                    
                    # åŠ è½½åŸºç¡€æ¨¡å‹
                    base_model = AutoModelForCausalLM.from_pretrained(
                        base_model_path,
                        local_files_only=True,
                        device_map="auto",
                        torch_dtype=torch.float32, # torch.float16 if torch.cuda.is_available() else torch.float32,
                        low_cpu_mem_usage=True,
                        trust_remote_code=True
                    )
                    
                    # åŠ è½½LoRA adapterå¹¶åˆå¹¶
                    model = PeftModel.from_pretrained(base_model, model_path)
                    model = model.merge_and_unload()
                    
                    # åŠ è½½åˆ†è¯å™¨
                    tokenizer = AutoTokenizer.from_pretrained(
                        base_model_path,
                        local_files_only=True,
                        trust_remote_code=True
                    )
                    
                except ImportError:
                    self.log("âŒ æœªå®‰è£…peftåº“ï¼Œæ— æ³•åŠ è½½LoRAæ¨¡å‹")
                    self.log("è¯·è¿è¡Œ: pip install peft")
                    return None
            else:
                # åŠ è½½å®Œæ•´æ¨¡å‹
                tokenizer = AutoTokenizer.from_pretrained(
                    model_path,
                    local_files_only=True,
                    trust_remote_code=True
                )
                
                model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    local_files_only=True,
                    device_map="auto",
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
            
            # ç¡®ä¿tokenizerè®¾ç½®æ­£ç¡®
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
                
            # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
            model.eval()
            
            # è¯»å–HumanEvalæ•°æ®é›†
            self.log("è¯»å–HumanEvalæ•°æ®é›†...")
            tasks = []
            with open(self.config["human_eval_path"], 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        tasks.append(json.loads(line))
            
            max_tasks = self.config.get("max_tasks", None)
            if max_tasks:
                tasks = tasks[:max_tasks]
                self.log(f"é™åˆ¶è¯„ä¼°ä»»åŠ¡æ•°: {max_tasks}")
            
            total_tasks = len(tasks)
            passed_tasks = 0
            failed_tasks = []
            detailed_results = []
            
            self.log(f"å¼€å§‹è¯„ä¼° {total_tasks} ä¸ªä»»åŠ¡...")
            start_time = time.time()
            
            for idx, task in enumerate(tasks, 1):
                task_id = task['task_id']
                prompt = task['prompt']
                entry_point = task['entry_point']
                test_code = task['test']
                
                # æ¯5ä¸ªä»»åŠ¡è¾“å‡ºä¸€æ¬¡è¿›åº¦
                if idx % 5 == 0 or idx == total_tasks:
                    elapsed = time.time() - start_time
                    rate = (passed_tasks / idx * 100) if idx > 0 else 0
                    self.log(f"è¿›åº¦: {idx}/{total_tasks} | é€šè¿‡: {passed_tasks} | é€šè¿‡ç‡: {rate:.1f}% | ç”¨æ—¶: {elapsed:.1f}ç§’")
                
                try:
                    # ç”Ÿæˆä»£ç 
                    messages = [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç»™å®šçš„å‡½æ•°ç­¾åå’Œæ–‡æ¡£å­—ç¬¦ä¸²ï¼Œå®ç°è¯¥å‡½æ•°ã€‚"},
                        {"role": "user", "content": prompt},
                    ]
                    
                    # åº”ç”¨èŠå¤©æ¨¡æ¿
                    try:
                        text = tokenizer.apply_chat_template(
                            messages,
                            tokenize=False,
                            add_generation_prompt=True
                        )
                    except:
                        # å¦‚æœèŠå¤©æ¨¡æ¿ä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•æ ¼å¼
                        text = f"ç³»ç»Ÿ: {messages[0]['content']}\nç”¨æˆ·: {messages[1]['content']}\nåŠ©æ‰‹: "
                    
                    # ç¼–ç è¾“å…¥
                    inputs = tokenizer(
                        text,
                        return_tensors="pt",
                        truncation=True,
                        max_length=1024
                    )
                    
                    # ç§»åˆ°GPU
                    if torch.cuda.is_available():
                        inputs = {k: v.cuda() for k, v in inputs.items()}
                    
                    # ç”Ÿæˆä»£ç 
                    with torch.no_grad():
                        generated_ids = model.generate(
                            **inputs,
                            max_new_tokens=self.config["max_tokens"],
                            temperature=self.config["temperature"],
                            top_p=self.config["top_p"],
                            do_sample=True,
                            pad_token_id=tokenizer.eos_token_id,
                            eos_token_id=tokenizer.eos_token_id,
                            num_beams=1
                        )
                    
                    # è§£ç ç”Ÿæˆçš„ä»£ç 
                    generated_tokens = generated_ids[0][inputs['input_ids'].shape[1]:]
                    generated_code = tokenizer.decode(generated_tokens, skip_special_tokens=True)
                    
                    # æå–å‡½æ•°ä»£ç 
                    function_code = self.extract_function_code(generated_code, entry_point)
                    
                    # æ„å»ºå®Œæ•´ä»£ç 
                    full_code = prompt + function_code + "\n" + test_code
                    
                    # æ‰§è¡Œæµ‹è¯•
                    test_result = self.run_code_test(full_code, entry_point)
                    
                    if test_result["passed"]:
                        passed_tasks += 1
                        detailed_results.append({
                            "task_id": task_id,
                            "status": "é€šè¿‡",
                            "error": None
                        })
                    else:
                        failed_tasks.append(task_id)
                        detailed_results.append({
                            "task_id": task_id,
                            "status": "å¤±è´¥",
                            "error": test_result.get("error", "æœªçŸ¥é”™è¯¯")
                        })
                        
                except Exception as e:
                    failed_tasks.append(task_id)
                    error_type = type(e).__name__
                    error_msg = str(e)[:100]
                    detailed_results.append({
                        "task_id": task_id,
                        "status": f"ç”Ÿæˆé”™è¯¯: {error_type}",
                        "error": error_msg
                    })
                
                # æ¸…ç†å†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # è®¡ç®—æœ€ç»ˆç»“æœ
            elapsed_time = time.time() - start_time
            pass_rate = (passed_tasks / total_tasks * 100) if total_tasks > 0 else 0
            
            result = {
                "model_name": model_name,
                "model_path": model_path,
                "total_tasks": total_tasks,
                "passed_tasks": passed_tasks,
                "failed_tasks_count": len(failed_tasks),
                "pass_rate": pass_rate,
                "elapsed_time": elapsed_time,
                "avg_time_per_task": elapsed_time / total_tasks if total_tasks > 0 else 0,
                "failed_task_ids": failed_tasks[:20],
                "detailed_results": detailed_results[:20],
                "evaluation_time": datetime.now().isoformat()
            }
            
            self.log(f"{model_name}è¯„ä¼°å®Œæˆ: {passed_tasks}/{total_tasks} é€šè¿‡ ({pass_rate:.2f}%)")
            
            # æ¸…ç†æ¨¡å‹
            del model, tokenizer
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            return result
            
        except Exception as e:
            self.log(f"è¯„ä¼°{model_name}æ—¶å‡ºé”™: {str(e)}")
            import traceback
            self.log(traceback.format_exc())
            return None
    
    def extract_function_code(self, generated_text, entry_point):
        """ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–å‡½æ•°ä»£ç """
        text = generated_text.strip()
        
        # æ–¹å¼1: æ­£åˆ™åŒ¹é…
        pattern = rf'def\s+{re.escape(entry_point)}\s*\([^)]*\)\s*:.*?(?=\n\ndef\s+|\nclass\s+|$)'
        match = re.search(pattern, text, re.DOTALL)
        if match:
            return match.group(0).strip()
        
        # æ–¹å¼2: åŸºäºç¼©è¿›
        if f"def {entry_point}" in text:
            lines = text.split('\n')
            start_idx = -1
            for i, line in enumerate(lines):
                if f"def {entry_point}" in line:
                    start_idx = i
                    break
            
            if start_idx >= 0:
                result = [lines[start_idx]]
                base_indent = len(lines[start_idx]) - len(lines[start_idx].lstrip())
                
                for i in range(start_idx + 1, len(lines)):
                    line = lines[i]
                    if not line.strip():
                        result.append(line)
                        continue
                    
                    current_indent = len(line) - len(line.lstrip())
                    if current_indent <= base_indent:
                        break
                    result.append(line)
                
                return '\n'.join(result)
        
        # æ–¹å¼3: è¿”å›æ•´ä¸ªæ–‡æœ¬
        return text
    
    def run_code_test(self, full_code, entry_point):
        """è¿è¡Œä»£ç æµ‹è¯•"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(full_code)
                temp_file = f.name
            
            # æ‰§è¡Œä»£ç 
            result = subprocess.run(
                ['python', temp_file],
                capture_output=True,
                text=True,
                timeout=5,
                env={**os.environ, 'PYTHONPATH': ''}
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_file)
            
            if result.returncode == 0:
                return {"passed": True}
            else:
                error_msg = result.stderr[:200] if result.stderr else "æœªçŸ¥é”™è¯¯"
                return {"passed": False, "error": error_msg}
                
        except subprocess.TimeoutExpired:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
            return {"passed": False, "error": "æ‰§è¡Œè¶…æ—¶"}
        except Exception as e:
            if os.path.exists(temp_file):
                os.unlink(temp_file)
            return {"passed": False, "error": str(e)}
    
    def compare_results(self, original_result, finetuned_result):
        """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„ç»“æœ"""
        orig_rate = original_result["pass_rate"]
        fine_rate = finetuned_result["pass_rate"]
        improvement = fine_rate - orig_rate
        
        orig_passed = set(original_result.get("failed_task_ids", []))
        fine_passed = set(finetuned_result.get("failed_task_ids", []))
        
        newly_passed = list(orig_passed - fine_passed)  # åŸæ¥å¤±è´¥ï¼Œç°åœ¨é€šè¿‡
        newly_failed = list(fine_passed - orig_passed)  # åŸæ¥é€šè¿‡ï¼Œç°åœ¨å¤±è´¥
        
        return {
            "improvement": improvement,
            "original_pass_rate": orig_rate,
            "finetuned_pass_rate": fine_rate,
            "newly_passed_tasks": newly_passed[:10],
            "newly_failed_tasks": newly_failed[:10],
            "original_total_tasks": original_result["total_tasks"],
            "finetuned_total_tasks": finetuned_result["total_tasks"],
            "original_passed": original_result["passed_tasks"],
            "finetuned_passed": finetuned_result["passed_tasks"]
        }

def start_evaluation_interface(config_data):
    """å¼€å§‹è¯„ä¼°ç•Œé¢å‡½æ•°"""
    global is_evaluating, evaluation_thread
    
    if is_evaluating:
        return "âš ï¸ è¯„ä¼°å·²ç»åœ¨è¿›è¡Œä¸­...", False
    
    # æ›´æ–°é…ç½®
    config = DEFAULT_CONFIG.copy()
    config.update(config_data)
    
    # æ£€æŸ¥å¿…è¦å‚æ•°
    required_fields = ["model_path", "finetuned_model_path", "human_eval_path"]
    for field in required_fields:
        if not config.get(field):
            return f"âŒ è¯·å¡«å†™{field}", False
    
    # æ£€æŸ¥è·¯å¾„
    for path_field in ["model_path", "finetuned_model_path", "human_eval_path"]:
        path = config[path_field]
        if not os.path.exists(path):
            return f"âŒ è·¯å¾„ä¸å­˜åœ¨: {path}", False
    
    # æ¸…ç©ºæ—¥å¿—
    log_collector.clear()
    
    # å¼€å§‹è¯„ä¼°çº¿ç¨‹
    evaluation_thread = EvaluationThread(config, log)
    is_evaluating = True
    evaluation_thread.start()
    
    start_msg = f"""
ğŸš€ å¼€å§‹æ¨¡å‹å¯¹æ¯”è¯„ä¼°...
åŸå§‹æ¨¡å‹: {config['model_path']}
å¾®è°ƒæ¨¡å‹: {config['finetuned_model_path']}
æ•°æ®é›†: {config['human_eval_path']}
æœ€å¤§ä»»åŠ¡æ•°: {config['max_tasks']}
    
è¯„ä¼°æ—¥å¿—å°†åœ¨ä¸‹æ–¹æ˜¾ç¤º...
    """
    
    log(start_msg)
    return "âœ… è¯„ä¼°å·²å¼€å§‹", True

def get_comparison_results():
    """è·å–å¯¹æ¯”ç»“æœ"""
    global comparison_results
    
    if not comparison_results:
        return "æš‚æ— è¯„ä¼°ç»“æœ"
    
    result_text = f"""
# æ¨¡å‹å¯¹æ¯”è¯„ä¼°ç»“æœ

## æ€»ä½“è¡¨ç°
- **åŸå§‹æ¨¡å‹é€šè¿‡ç‡**: {comparison_results['original_pass_rate']:.2f}%
- **å¾®è°ƒæ¨¡å‹é€šè¿‡ç‡**: {comparison_results['finetuned_pass_rate']:.2f}%
- **æå‡æ•ˆæœ**: {comparison_results['improvement']:+.2f}%

## è¯¦ç»†æ•°æ®
- åŸå§‹æ¨¡å‹: {comparison_results['original_passed']}/{comparison_results['original_total_tasks']} é€šè¿‡
- å¾®è°ƒæ¨¡å‹: {comparison_results['finetuned_passed']}/{comparison_results['finetuned_total_tasks']} é€šè¿‡

## æ”¹è¿›åˆ†æ
"""
    
    if comparison_results['newly_passed_tasks']:
        result_text += f"- **æ–°é€šè¿‡çš„ä»»åŠ¡**: {len(comparison_results['newly_passed_tasks'])} ä¸ª\n"
        if comparison_results['newly_passed_tasks']:
            result_text += f"  ç¤ºä¾‹: {', '.join(comparison_results['newly_passed_tasks'][:5])}\n"
    
    if comparison_results['newly_failed_tasks']:
        result_text += f"- **æ–°å¤±è´¥çš„ä»»åŠ¡**: {len(comparison_results['newly_failed_tasks'])} ä¸ª\n"
        if comparison_results['newly_failed_tasks']:
            result_text += f"  ç¤ºä¾‹: {', '.join(comparison_results['newly_failed_tasks'][:5])}\n"
    
    if comparison_results['improvement'] > 0:
        result_text += "\nğŸ‰ **å¾®è°ƒæ•ˆæœ: æå‡æ˜æ˜¾**"
    elif comparison_results['improvement'] == 0:
        result_text += "\nâš ï¸ **å¾®è°ƒæ•ˆæœ: æ— æ˜æ˜¾å˜åŒ–**"
    else:
        result_text += "\nâŒ **å¾®è°ƒæ•ˆæœ: æ€§èƒ½ä¸‹é™**"
    
    return result_text

# ====== å·¥å…·å‡½æ•° ======
def check_paths(model_path, finetuned_model_path, human_eval_path):
    """æ£€æŸ¥è·¯å¾„"""
    results = []
    
    # æ£€æŸ¥åŸå§‹æ¨¡å‹
    if os.path.exists(model_path):
        results.append(f"âœ… åŸå§‹æ¨¡å‹è·¯å¾„å­˜åœ¨: {model_path}")
    else:
        results.append(f"âŒ åŸå§‹æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
    
    # æ£€æŸ¥å¾®è°ƒæ¨¡å‹
    if os.path.exists(finetuned_model_path):
        results.append(f"âœ… å¾®è°ƒæ¨¡å‹è·¯å¾„å­˜åœ¨: {finetuned_model_path}")
    else:
        results.append(f"âŒ å¾®è°ƒæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {finetuned_model_path}")
    
    # æ£€æŸ¥æ•°æ®é›†
    if os.path.exists(human_eval_path):
        results.append(f"âœ… HumanEvalæ•°æ®é›†å­˜åœ¨: {human_eval_path}")
    else:
        results.append(f"âŒ HumanEvalæ•°æ®é›†ä¸å­˜åœ¨: {human_eval_path}")
        results.append("è¯·ä» https://github.com/openai/human-eval ä¸‹è½½æ•°æ®é›†")
    
    return "\n".join(results)

def generate_example_dataset(instructions):
    """ç”Ÿæˆç¤ºä¾‹æ•°æ®é›†"""
    lines = instructions.strip().split('\n')
    dataset = []
    
    for i, instr in enumerate(lines):
        if instr.strip():
            # ç®€å•ç”Ÿæˆå¯¹åº”çš„ä»£ç 
            if "add" in instr.lower() and "number" in instr.lower():
                code = """def add_numbers(a, b):
    \"\"\"Add two numbers\"\"\"
    return a + b"""
            elif "prime" in instr.lower():
                code = """def is_prime(n):
    \"\"\"Check if a number is prime\"\"\"
    if n <= 1:
        return False
    for i in range(2, int(n**0.5) + 1):
        if n % i == 0:
            return False
    return True"""
            else:
                code = f"""def function_{i}():
    \"\"\"Generated function\"\"\"
    # TODO: Implement this function
    pass"""
            
            dataset.append({
                "instruction": instr.strip(),
                "code": code
            })
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    output_path = "./example_dataset.jsonl"
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in dataset:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    return f"âœ… ç¤ºä¾‹æ•°æ®é›†å·²ç”Ÿæˆ: {output_path}\nå…± {len(dataset)} ä¸ªæ ·æœ¬"

def update_system_info():
    """æ›´æ–°ç³»ç»Ÿä¿¡æ¯"""
    gpu_text = ""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        gpu_text = f"âœ… GPUå¯ç”¨\nåç§°: {gpu_name}\næ˜¾å­˜: {gpu_memory:.1f} GB"
    else:
        gpu_text = "âŒ æœªæ£€æµ‹åˆ°GPU\nå°†åœ¨CPUä¸Šè¿è¡Œï¼Œé€Ÿåº¦è¾ƒæ…¢"
    
    model_text = "âŒ æ¨¡å‹æœªåŠ è½½"
    global model
    if model is not None:
        model_text = "âœ… æ¨¡å‹å·²åŠ è½½\nå¯ä½¿ç”¨ç”Ÿæˆå’Œå¾®è°ƒåŠŸèƒ½"
    
    return gpu_text, model_text

def update_logs():
    """æ›´æ–°æ—¥å¿—æ˜¾ç¤º"""
    logs = log_collector.get_logs(50)
    return logs

# ====== åˆ›å»ºå®Œæ•´çš„Gradioç•Œé¢ ======
with gr.Blocks(title="Qwen2.5-Coder å®Œæ•´ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤– Qwen2.5-Coder å®Œæ•´ç³»ç»Ÿ")
    gr.Markdown("æ¨¡å‹åŠ è½½ã€å¾®è°ƒã€è¯„ä¼°ä¸€ä½“åŒ–ç³»ç»Ÿ")
    
    # æ·»åŠ ä¸€ä¸ªéšè—çš„çŠ¶æ€ç»„ä»¶æ¥å­˜å‚¨è®­ç»ƒé…ç½®
    training_config_state = gr.State({})

    # çŠ¶æ€æ˜¾ç¤º
    with gr.Row():
        status_display = gr.Textbox(
            label="ç³»ç»ŸçŠ¶æ€",
            value="å‡†å¤‡å°±ç»ª",
            interactive=False,
            lines=1,
            scale=3
        )
        
        clear_logs_btn = gr.Button("æ¸…ç©ºæ—¥å¿—", variant="secondary", size="sm", scale=1)
    
    # æ—¥å¿—è¾“å‡ºåŒºåŸŸ
    log_output = gr.Textbox(
        label="ç³»ç»Ÿæ—¥å¿—",
        value="æ¬¢è¿ä½¿ç”¨ Qwen2.5-Coder å®Œæ•´ç³»ç»Ÿ\n\nè¯·é€‰æ‹©é€‰é¡¹å¡å¼€å§‹æ“ä½œã€‚",
        lines=25,
        interactive=False
    )
    
    # ç³»ç»Ÿä¿¡æ¯
    with gr.Row():
        gpu_info = gr.Textbox(
            label="GPUçŠ¶æ€",
            value="æ­£åœ¨æ£€æµ‹...",
            lines=2,
            interactive=False,
            scale=1
        )
        
        model_info = gr.Textbox(
            label="æ¨¡å‹ä¿¡æ¯",
            value="æœªåŠ è½½",
            lines=2,
            interactive=False,
            scale=1
        )
        
        results_info = gr.Textbox(
            label="æœ€æ–°ç»“æœ",
            value="æš‚æ— ç»“æœ",
            lines=2,
            interactive=False,
            scale=1
        )
    
    # é€‰é¡¹å¡
    with gr.Tabs():
        # ====== Tab 1: æ¨¡å‹åŠ è½½ ======
        with gr.TabItem("ğŸš€ æ¨¡å‹åŠ è½½"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### âš™ï¸ æ¨¡å‹é…ç½®")
                    
                    model_path = gr.Textbox(
                        label="æ¨¡å‹è·¯å¾„",
                        value=DEFAULT_CONFIG["model_path"],
                        placeholder="è¾“å…¥æœ¬åœ°æ¨¡å‹è·¯å¾„",
                        lines=1
                    )
                    
                    load_btn = gr.Button(
                        "ğŸš€ åŠ è½½æ¨¡å‹",
                        variant="primary",
                        size="lg"
                    )
                    
                    load_status = gr.Textbox(
                        label="åŠ è½½çŠ¶æ€",
                        value="ç­‰å¾…åŠ è½½",
                        interactive=False,
                        lines=3
                    )
                    
                    gr.Markdown("### ğŸ“Š ç³»ç»Ÿä¿¡æ¯")
                    
                    with gr.Accordion("è®¾å¤‡ä¿¡æ¯", open=True):
                        gr.Markdown("""
                        - **CPU**: Python è¿è¡Œæ—¶
                        - **GPU**: è‡ªåŠ¨æ£€æµ‹å¯ç”¨æ€§
                        - **å†…å­˜**: æ ¹æ®é…ç½®è°ƒæ•´
                        - **æ˜¾å­˜**: è®­ç»ƒæ—¶éœ€è¦è¶³å¤Ÿæ˜¾å­˜
                        """)
                
                with gr.Column(scale=2):
                    gr.Markdown("### ğŸ“ åŠ è½½è¯´æ˜")
                    
                    with gr.Accordion("è¯¦ç»†è¯´æ˜", open=True):
                        gr.Markdown("""
                        ### æ¨¡å‹åŠ è½½æ­¥éª¤ï¼š
                        
                        1. **å‡†å¤‡æ¨¡å‹æ–‡ä»¶**
                           - ä¸‹è½½ Qwen2.5-Coder-0.5B-Instruct æ¨¡å‹
                           - æ¨¡å‹åº”åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š
                             - config.json
                             - pytorch_model.bin
                             - tokenizer_config.json
                             - tokenizer.json
                        
                        2. **é…ç½®æ¨¡å‹è·¯å¾„**
                           - è¾“å…¥æ¨¡å‹æ‰€åœ¨çš„æœ¬åœ°è·¯å¾„
                           - ç¤ºä¾‹: `./models/Qwen2.5-Coder-0.5B-Instruct`
                        
                        3. **åŠ è½½æ¨¡å‹**
                           - ç‚¹å‡»"åŠ è½½æ¨¡å‹"æŒ‰é’®
                           - ç³»ç»Ÿä¼šéªŒè¯æ¨¡å‹æ–‡ä»¶
                           - åŠ è½½åˆ°å¯ç”¨è®¾å¤‡ï¼ˆGPU/CPUï¼‰
                           - æ˜¾ç¤ºåŠ è½½çŠ¶æ€
                        
                        ### æ¨¡å‹ä¿¡æ¯ï¼š
                        - **å‚æ•°è§„æ¨¡**: 5äº¿å‚æ•°
                        - **æ¨¡å‹ç±»å‹**: æŒ‡ä»¤å¾®è°ƒç‰ˆæœ¬
                        - **é€‚ç”¨ä»»åŠ¡**: ä»£ç ç”Ÿæˆã€ä»£ç è§£é‡Š
                        - **æ”¯æŒè¯­è¨€**: Pythonä¸ºä¸»ï¼Œæ”¯æŒå…¶ä»–è¯­è¨€
                        
                        ### æ³¨æ„äº‹é¡¹ï¼š
                        - é¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´
                        - GPUåŠ è½½é€Ÿåº¦æ›´å¿«
                        - ç¡®ä¿æ¨¡å‹æ–‡ä»¶å®Œæ•´
                        """)
        
        # ====== Tab 2: æ¨¡å‹å¾®è°ƒï¼ˆå·²ä¿®æ”¹ä¸ºä½¿ç”¨MBPPï¼‰ ======
        with gr.TabItem("ğŸ¯ æ¨¡å‹å¾®è°ƒ"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### âš™ï¸ è®­ç»ƒé…ç½®")
                    
                    mbpp_dataset_path = gr.Textbox(
                        label="MBPPæ•°æ®é›†è·¯å¾„",
                        value=DEFAULT_CONFIG["mbpp_dataset_path"],
                        placeholder="mbpp_text_only.jsonl è·¯å¾„",
                        lines=1
                    )
                    
                    output_dir = gr.Textbox(
                        label="è¾“å‡ºç›®å½•",
                        value=DEFAULT_CONFIG["output_dir"],
                        placeholder="å¾®è°ƒåæ¨¡å‹çš„ä¿å­˜è·¯å¾„",
                        lines=1
                    )
                    
                    with gr.Row():
                        num_epochs = gr.Number(
                            label="è®­ç»ƒè½®æ•°",
                            value=DEFAULT_CONFIG["num_epochs"],
                            minimum=1,
                            maximum=10,
                            step=1
                        )
                        
                        learning_rate = gr.Number(
                            label="å­¦ä¹ ç‡",
                            value=DEFAULT_CONFIG["learning_rate"],
                            minimum=1e-6,
                            maximum=1e-2,
                            step=1e-6
                        )
                    
                    batch_size = gr.Slider(
                        label="æ‰¹å¤§å°",
                        value=DEFAULT_CONFIG["batch_size"],
                        minimum=1,
                        maximum=16,
                        step=1
                    )
                    
                    max_generate_items = gr.Number(
                        label="æœ€å¤§ç”Ÿæˆæ•°æ®é‡",
                        value=DEFAULT_CONFIG["max_generate_items"],
                        minimum=10,
                        maximum=500,
                        step=10,
                        info="ä»MBPPç”Ÿæˆå¤šå°‘æ¡è®­ç»ƒæ•°æ®"
                    )
                    
                    with gr.Row():
                        use_lora = gr.Checkbox(
                            label="ä½¿ç”¨LoRA",
                            value=DEFAULT_CONFIG["use_lora"]
                        )
                        use_4bit = gr.Checkbox(
                            label="4-bité‡åŒ–",
                            value=DEFAULT_CONFIG["use_4bit"]
                        )
                    
                    train_btn = gr.Button(
                        "ğŸ¯ å¼€å§‹å¾®è°ƒ",
                        variant="stop",
                        size="lg"
                    )
                    
                    train_status = gr.Textbox(
                        label="è®­ç»ƒçŠ¶æ€",
                        value="ç­‰å¾…å¼€å§‹",
                        interactive=False,
                        lines=3
                    )
                    
                    gr.Markdown("---")
                    gr.Markdown("### ğŸ› ï¸ å·¥å…·")
                    
                    with gr.Accordion("æ£€æŸ¥MBPPæ•°æ®é›†", open=False):
                        check_mbpp_btn = gr.Button("æ£€æŸ¥MBPPæ•°æ®é›†", variant="secondary")
                        check_mbpp_output = gr.Textbox(label="æ£€æŸ¥ç»“æœ", interactive=False, lines=3)
                    
                    with gr.Accordion("ç”Ÿæˆç¤ºä¾‹æ•°æ®é›†", open=False):
                        example_instructions = gr.Textbox(
                            label="ç¤ºä¾‹æŒ‡ä»¤ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
                            value="Write a function to add two numbers\nWrite a function to check if a number is prime",
                            lines=5
                        )
                        
                        generate_btn = gr.Button("ç”Ÿæˆç¤ºä¾‹æ•°æ®é›†", variant="secondary")
                        generate_output = gr.Textbox(label="ç”Ÿæˆç»“æœ", interactive=False)
                
                with gr.Column(scale=2):
                    gr.Markdown("### ğŸ“ å¾®è°ƒè¯´æ˜")
                    
                    with gr.Accordion("è¯¦ç»†è¯´æ˜", open=True):
                        gr.Markdown("""
                        ### æ–°çš„è®­ç»ƒæµç¨‹ï¼ˆä½¿ç”¨MBPPæ•°æ®é›†ï¼‰ï¼š
                        
                        1. **å‡†å¤‡MBPPæ•°æ®é›†**
                           - æ•°æ®é›†åº”ä¸º mbpp_text_only.jsonl æ ¼å¼
                           - æ¯è¡Œæ˜¯ä¸€ä¸ªæŒ‡ä»¤å­—ç¬¦ä¸²ï¼Œç”¨åŒå¼•å·åŒ…è£¹
                           - ç¤ºä¾‹: `"Write a function to find the minimum cost path..."`
                        
                        2. **ç¬¬ä¸€é˜¶æ®µï¼šç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®**
                           - ç³»ç»Ÿä¼šè‡ªåŠ¨è°ƒç”¨Qwen APIç”Ÿæˆä»£ç 
                           - ä½¿ç”¨32Bæ¨¡å‹ç”Ÿæˆä»£ç ï¼Œ14Bæ¨¡å‹éªŒè¯é€»è¾‘
                           - è¿›è¡Œè¯­æ³•æ£€æŸ¥å’ŒåŸºæœ¬æµ‹è¯•
                           - åªæœ‰é€šè¿‡éªŒè¯çš„æŒ‡ä»¤-ä»£ç å¯¹æ‰ä¼šè¢«ä¿ç•™
                        
                        3. **ç¬¬äºŒé˜¶æ®µï¼šæ¨¡å‹å¾®è°ƒ**
                           - ä½¿ç”¨ç”Ÿæˆçš„é«˜è´¨é‡æ•°æ®è¿›è¡Œå¾®è°ƒ
                           - æ”¯æŒLoRAå’Œ4-bité‡åŒ–ä»¥èŠ‚çœæ˜¾å­˜
                           - è®­ç»ƒå®Œæˆåè‡ªåŠ¨ä¿å­˜æ¨¡å‹
                        
                        ### è®­ç»ƒå‚æ•°ï¼š
                        - **è®­ç»ƒè½®æ•°**: 1-10è½®ï¼Œé€šå¸¸3è½®è¶³å¤Ÿ
                        - **å­¦ä¹ ç‡**: 2e-4 æ˜¯æ¯”è¾ƒåˆé€‚çš„åˆå§‹å€¼
                        - **æ‰¹å¤§å°**: æ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼Œ4-8æ¯”è¾ƒå¸¸è§
                        - **æœ€å¤§ç”Ÿæˆæ•°æ®é‡**: æ§åˆ¶ä»MBPPç”Ÿæˆçš„æ ·æœ¬æ•°é‡
                        - **LoRA**: ä½ç§©é€‚é…ï¼Œå‡å°‘è®­ç»ƒå‚æ•°
                        - **4-bité‡åŒ–**: å‡å°‘æ˜¾å­˜ä½¿ç”¨
                        
                        ### ç¡¬ä»¶è¦æ±‚ï¼š
                        - **GPUæ¨è**: è‡³å°‘8GBæ˜¾å­˜ï¼ˆRTX 3070/4060 TiåŠä»¥ä¸Šï¼‰
                        - **CPUæ¨¡å¼**: å¯ä»¥è¿è¡Œä½†é€Ÿåº¦è¾ƒæ…¢
                        - **å†…å­˜**: è‡³å°‘16GB RAM
                        
                        ### è®­ç»ƒæ—¶é—´ï¼š
                        - æ•°æ®ç”Ÿæˆé˜¶æ®µï¼šæ¯50æ¡æ•°æ®çº¦15-30åˆ†é’Ÿï¼ˆä¾èµ–APIé€Ÿåº¦ï¼‰
                        - å¾®è°ƒé˜¶æ®µï¼š100ä¸ªæ ·æœ¬çº¦5-10åˆ†é’Ÿ
                        
                        ### æ³¨æ„äº‹é¡¹ï¼š
                        - è®­ç»ƒè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼Œè¯·è€å¿ƒç­‰å¾…
                        - éœ€è¦æœ‰æ•ˆçš„APIå¯†é’¥æ‰èƒ½ç”Ÿæˆè®­ç»ƒæ•°æ®
                        - è®­ç»ƒè¿‡ç¨‹ä¸­è¯·ä¸è¦å…³é—­ç½‘é¡µ
                        """)
        
        # ====== Tab 3: æ¨¡å‹è¯„ä¼° ======
        with gr.TabItem("ğŸ“Š æ¨¡å‹è¯„ä¼°"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### âš™ï¸ è¯„ä¼°é…ç½®")
                    
                    finetuned_model_path = gr.Textbox(
                        label="å¾®è°ƒæ¨¡å‹è·¯å¾„",
                        value=DEFAULT_CONFIG["finetuned_model_path"],
                        placeholder="å¾®è°ƒåæ¨¡å‹è·¯å¾„",
                        lines=1
                    )
                    
                    human_eval_path = gr.Textbox(
                        label="HumanEvalæ•°æ®é›†è·¯å¾„",
                        value=DEFAULT_CONFIG["human_eval_path"],
                        placeholder="human-eval-v2-20210705.jsonl è·¯å¾„",
                        lines=1
                    )
                    
                    with gr.Row():
                        max_tasks = gr.Number(
                            label="æœ€å¤§ä»»åŠ¡æ•°",
                            value=DEFAULT_CONFIG["max_tasks"],
                            minimum=1,
                            maximum=164,
                            step=1,
                            info="HumanEvalå…±164ä¸ªä»»åŠ¡"
                        )
                        
                        max_tokens = gr.Number(
                            label="æœ€å¤§ç”Ÿæˆtokenæ•°",
                            value=DEFAULT_CONFIG["max_tokens"],
                            minimum=50,
                            maximum=2048,
                            step=50
                        )
                    
                    with gr.Row():
                        temperature = gr.Slider(
                            label="Temperature",
                            value=DEFAULT_CONFIG["temperature"],
                            minimum=0.1,
                            maximum=2.0,
                            step=0.1
                        )
                        
                        top_p = gr.Slider(
                            label="Top-p",
                            value=DEFAULT_CONFIG["top_p"],
                            minimum=0.1,
                            maximum=1.0,
                            step=0.05
                        )
                    
                    eval_btn = gr.Button(
                        "ğŸš€ å¼€å§‹è¯„ä¼°",
                        variant="primary",
                        size="lg"
                    )
                    
                    eval_status = gr.Textbox(
                        label="è¯„ä¼°çŠ¶æ€",
                        value="ç­‰å¾…å¼€å§‹",
                        interactive=False,
                        lines=3
                    )
                    
                    gr.Markdown("---")
                    gr.Markdown("### ğŸ“ˆ è¯„ä¼°ç»“æœ")
                    
                    results_btn = gr.Button(
                        "ğŸ“Š æŸ¥çœ‹ç»“æœ",
                        variant="secondary",
                        size="lg"
                    )
                    
                    check_btn = gr.Button("æ£€æŸ¥è·¯å¾„", variant="secondary")
                    check_output = gr.Textbox(label="è·¯å¾„æ£€æŸ¥ç»“æœ", interactive=False, lines=5)
                
                with gr.Column(scale=2):
                    gr.Markdown("### ğŸ“ è¯„ä¼°è¯´æ˜")
                    
                    with gr.Accordion("è¯¦ç»†è¯´æ˜", open=True):
                        gr.Markdown("""
                        ### è¯„ä¼°æµç¨‹ï¼š
                        
                        1. **é…ç½®è¯„ä¼°å‚æ•°**
                           - åŸå§‹æ¨¡å‹è·¯å¾„ï¼šè‡ªåŠ¨ä½¿ç”¨å·²åŠ è½½çš„æ¨¡å‹
                           - å¾®è°ƒæ¨¡å‹è·¯å¾„ï¼šå¾®è°ƒåçš„æ¨¡å‹ä¿å­˜è·¯å¾„
                           - HumanEvalæ•°æ®é›†ï¼šä»GitHubä¸‹è½½çš„jsonlæ–‡ä»¶
                        
                        2. **è°ƒæ•´è¯„ä¼°å‚æ•°**
                           - æœ€å¤§ä»»åŠ¡æ•°ï¼šå»ºè®®å…ˆè¯„ä¼°20-50ä¸ªä»»åŠ¡æµ‹è¯•
                           - ç”Ÿæˆå‚æ•°ï¼štemperatureå’Œtop-på½±å“ä»£ç å¤šæ ·æ€§
                        
                        3. **å¼€å§‹è¯„ä¼°**
                           - ç‚¹å‡»"å¼€å§‹è¯„ä¼°"æŒ‰é’®
                           - ç³»ç»Ÿä¼šä¾æ¬¡è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹
                           - æ¯ä¸ªä»»åŠ¡æ‰§è¡Œä»£ç æµ‹è¯•
                           - å®æ—¶æ˜¾ç¤ºè¯„ä¼°æ—¥å¿—
                        
                        4. **æŸ¥çœ‹ç»“æœ**
                           - è¯„ä¼°å®Œæˆåç‚¹å‡»"æŸ¥çœ‹ç»“æœ"
                           - æ˜¾ç¤ºä¸¤ä¸ªæ¨¡å‹çš„å¯¹æ¯”æ•°æ®
                           - åˆ†æå¾®è°ƒæ•ˆæœ
                        
                        ### HumanEvalæ•°æ®é›†ï¼š
                        - åŒ…å«164ä¸ªPythonç¼–ç¨‹ä»»åŠ¡
                        - æ¯ä¸ªä»»åŠ¡æœ‰å¤šä¸ªæµ‹è¯•ç”¨ä¾‹
                        - åªæœ‰é€šè¿‡æ‰€æœ‰æµ‹è¯•æ‰ç®—é€šè¿‡
                        - æ•°æ®é›†ä¸‹è½½ï¼šhttps://github.com/openai/human-eval
                        
                        ### è¯„ä¼°æŒ‡æ ‡ï¼š
                        - **é€šè¿‡ç‡**ï¼šé€šè¿‡çš„ä»»åŠ¡æ•° / æ€»ä»»åŠ¡æ•°
                        - **æå‡æ•ˆæœ**ï¼šå¾®è°ƒåé€šè¿‡ç‡ - åŸå§‹é€šè¿‡ç‡
                        - **æ–°é€šè¿‡ä»»åŠ¡**ï¼šåŸæ¥å¤±è´¥ï¼Œå¾®è°ƒåé€šè¿‡çš„ä»»åŠ¡
                        - **æ–°å¤±è´¥ä»»åŠ¡**ï¼šåŸæ¥é€šè¿‡ï¼Œå¾®è°ƒåå¤±è´¥çš„ä»»åŠ¡
                        
                        ### è¯„ä¼°æ—¶é—´ï¼š
                        - 20ä¸ªä»»åŠ¡ï¼šæ¯ä¸ªæ¨¡å‹çº¦5-10åˆ†é’Ÿ
                        - 50ä¸ªä»»åŠ¡ï¼šæ¯ä¸ªæ¨¡å‹çº¦15-25åˆ†é’Ÿ
                        - 164ä¸ªä»»åŠ¡ï¼šæ¯ä¸ªæ¨¡å‹çº¦45-90åˆ†é’Ÿ
                        
                        ### æ³¨æ„äº‹é¡¹ï¼š
                        - è¯„ä¼°éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
                        - éœ€è¦è¶³å¤Ÿçš„GPUæ˜¾å­˜ï¼ˆè‡³å°‘8GBï¼‰
                        - ç¡®ä¿æ¨¡å‹è·¯å¾„å’Œæ•°æ®é›†è·¯å¾„æ­£ç¡®
                        """)
        
        # ====== Tab 4: å¤§æ¨¡å‹é—®ç­”ï¼ˆæ–°å¢ï¼‰ ======
        with gr.TabItem("ğŸ’¬ å¤§æ¨¡å‹é—®ç­”"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("### ğŸ’¬ æ¨¡å‹é—®ç­”")
                    
                    with gr.Row():
                        instruction_input = gr.Textbox(
                            label="è¾“å…¥æŒ‡ä»¤",
                            placeholder="ä¾‹å¦‚ï¼šWrite a function to add two numbers",
                            lines=3,
                            max_lines=5,
                            scale=3
                        )
                        
                        start_qa_btn = gr.Button(
                            "ğŸš€ å¼€å§‹",
                            variant="primary",
                            size="lg",
                            scale=1
                        )
                    
                    with gr.Row():
                        with gr.Column():
                            gen_temperature = gr.Slider(
                                label="Temperature",
                                value=DEFAULT_CONFIG["gen_temperature"],
                                minimum=0.1,
                                maximum=1.5,
                                step=0.1
                            )
                            
                            gen_top_p = gr.Slider(
                                label="Top-p",
                                value=DEFAULT_CONFIG["gen_top_p"],
                                minimum=0.1,
                                maximum=1.0,
                                step=0.05
                            )
                        
                        with gr.Column():
                            max_new_tokens = gr.Number(
                                label="æœ€å¤§ç”Ÿæˆtokenæ•°",
                                value=DEFAULT_CONFIG["max_new_tokens"],
                                minimum=50,
                                maximum=2048,
                                step=50
                            )
                    
                    code_output = gr.Code(
                        label="ç”Ÿæˆçš„ä»£ç ",
                        language="python",
                        lines=15,
                        interactive=False
                    )
                    
                    save_status = gr.Textbox(
                        label="ä¿å­˜çŠ¶æ€",
                        value="ç­‰å¾…ç”Ÿæˆ",
                        interactive=False,
                        lines=2
                    )
                    
                    qa_status = gr.Textbox(
                        label="é—®ç­”çŠ¶æ€",
                        value="ç­‰å¾…è¾“å…¥",
                        interactive=False,
                        lines=3
                    )
                    
                    gr.Markdown("---")
                    gr.Markdown("### ğŸ› ï¸ å·¥å…·")
                    
                    with gr.Accordion("ç¤ºä¾‹æŒ‡ä»¤", open=False):
                        example_instr1 = gr.Button("ç¤ºä¾‹1: ä¸¤æ•°ç›¸åŠ ", variant="secondary", size="sm")
                        example_instr2 = gr.Button("ç¤ºä¾‹2: åˆ¤æ–­è´¨æ•°", variant="secondary", size="sm")
                        example_instr3 = gr.Button("ç¤ºä¾‹3: æ–æ³¢é‚£å¥‘æ•°åˆ—", variant="secondary", size="sm")
                        example_instr4 = gr.Button("ç‰¹æ®ŠæŒ‡ä»¤: è‡ªæˆ‘æ¼”åŒ–", variant="stop", size="sm")
                
                with gr.Column(scale=2):
                    gr.Markdown("### ğŸ“ ä½¿ç”¨è¯´æ˜")
                    
                    with gr.Accordion("è¯¦ç»†è¯´æ˜", open=True):
                        gr.Markdown("""
                        ### å¤§æ¨¡å‹é—®ç­”åŠŸèƒ½ï¼š
                        
                        1. **åŸºæœ¬åŠŸèƒ½**ï¼š
                           - è¾“å…¥ç¼–ç¨‹ç›¸å…³çš„æŒ‡ä»¤ï¼ˆè‹±æ–‡/ä¸­æ–‡å‡å¯ï¼‰
                           - ç‚¹å‡»"å¼€å§‹"æŒ‰é’®ç”Ÿæˆä»£ç 
                           - ç”Ÿæˆçš„ä»£ç ä¼šæ˜¾ç¤ºåœ¨å³ä¾§
                           - æŒ‡ä»¤ä¼šè‡ªåŠ¨ä¿å­˜åˆ°MBPPæ•°æ®é›†ä¸­
                        
                        2. **ç‰¹æ®ŠåŠŸèƒ½ï¼šè‡ªæˆ‘æ¼”åŒ–**ï¼š
                           - è¾“å…¥"è‡ªæˆ‘æ¼”åŒ–"ï¼ˆä¸å¸¦å¼•å·ï¼‰
                           - ç³»ç»Ÿä¼šè‡ªåŠ¨å¼€å§‹å¾®è°ƒæµç¨‹
                           - ä½¿ç”¨å·²ä¿å­˜çš„æŒ‡ä»¤è¿›è¡Œè®­ç»ƒ
                           - å®ç°æ¨¡å‹çš„è‡ªæˆ‘æ”¹è¿›
                        
                        3. **å‚æ•°è¯´æ˜**ï¼š
                           - **Temperature**ï¼šæ§åˆ¶ç”Ÿæˆéšæœºæ€§ï¼Œå€¼è¶Šé«˜è¶Šéšæœº
                           - **Top-p**ï¼šæ ¸é‡‡æ ·å‚æ•°ï¼Œæ§åˆ¶å€™é€‰è¯çš„é€‰æ‹©èŒƒå›´
                           - **æœ€å¤§ç”Ÿæˆtokenæ•°**ï¼šé™åˆ¶ç”Ÿæˆä»£ç çš„é•¿åº¦
                        
                        4. **æŒ‡ä»¤ä¿å­˜**ï¼š
                           - æ¯æ¬¡æˆåŠŸé—®ç­”åï¼ŒæŒ‡ä»¤ä¼šè‡ªåŠ¨ä¿å­˜åˆ°MBPPæ•°æ®é›†
                           - ä¿å­˜æ ¼å¼ï¼šç”¨åŒå¼•å·åŒ…è£¹çš„å­—ç¬¦ä¸²
                           - æ–‡ä»¶è·¯å¾„ï¼š`./datasets/mbpp_text_only.jsonl`
                        
                        5. **æ¨¡å‹è¦æ±‚**ï¼š
                           - éœ€è¦å…ˆåŠ è½½0.5Bæ¨¡å‹ï¼ˆåœ¨"æ¨¡å‹åŠ è½½"é€‰é¡¹å¡ï¼‰
                           - æ¨¡å‹æœªåŠ è½½æ—¶ä¼šæç¤ºé”™è¯¯
                           - å»ºè®®ä½¿ç”¨GPUä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆé€Ÿåº¦
                        
                        6. **ä½¿ç”¨æŠ€å·§**ï¼š
                           - å¯¹äºå¤æ‚æŒ‡ä»¤ï¼Œå¯ä»¥å¢åŠ æœ€å¤§ç”Ÿæˆtokenæ•°
                           - å¯¹äºåˆ›é€ æ€§ä»»åŠ¡ï¼Œå¯ä»¥é€‚å½“æé«˜Temperature
                           - å¯¹äºç¡®å®šæ€§ä»»åŠ¡ï¼Œå¯ä»¥é™ä½Temperature
                           - å®šæœŸæ‰§è¡Œ"è‡ªæˆ‘æ¼”åŒ–"å¯ä»¥æå‡æ¨¡å‹æ€§èƒ½
                        
                        7. **æ³¨æ„äº‹é¡¹**ï¼š
                           - ç”Ÿæˆä»£ç å¯èƒ½éœ€è¦å‡ ç§’åˆ°å‡ åç§’æ—¶é—´
                           - æŒ‡ä»¤ä¿å­˜ä¸å½±å“ç°æœ‰MBPPæ•°æ®
                           - "è‡ªæˆ‘æ¼”åŒ–"ä¼šè§¦å‘å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼Œè€—æ—¶è¾ƒé•¿
                           - ç¡®ä¿æ¨¡å‹å·²åŠ è½½æ‰èƒ½ä½¿ç”¨æ­¤åŠŸèƒ½
                        """)
    
    # ====== äº‹ä»¶ç»‘å®š ======
    
    # æ¸…ç©ºæ—¥å¿—
    def clear_logs():
        log_collector.clear()
        return "æ—¥å¿—å·²æ¸…ç©º"
    
    clear_logs_btn.click(
        fn=clear_logs,
        outputs=log_output
    )
    
    # Tab 1: æ¨¡å‹åŠ è½½
    load_btn.click(
        fn=load_model_interface,
        inputs=[model_path],
        outputs=[load_status, status_display]
    ).then(
        fn=update_system_info,
        outputs=[gpu_info, model_info]
    )
    
    # Tab 2: æ¨¡å‹å¾®è°ƒ
    def collect_training_config(
        mbpp_dataset_path_val, output_dir_val, 
        num_epochs_val, learning_rate_val,
        batch_size_val, max_generate_items_val, use_lora_val, use_4bit_val
    ):
        # ä»æ¨¡å‹åŠ è½½é€‰é¡¹å¡è·å–æ¨¡å‹è·¯å¾„
        model_path_val = model_path.value if hasattr(model_path, 'value') else DEFAULT_CONFIG["model_path"]
        
        config = {
            "model_path": model_path_val,
            "mbpp_dataset_path": mbpp_dataset_path_val,
            "output_dir": output_dir_val,
            "num_epochs": int(num_epochs_val),
            "learning_rate": float(learning_rate_val),
            "batch_size": int(batch_size_val),
            "max_generate_items": int(max_generate_items_val),
            "use_lora": use_lora_val,
            "use_4bit": use_4bit_val
        }
        return config
    
    train_btn.click(
        fn=collect_training_config,
        inputs=[
            mbpp_dataset_path, output_dir,
            num_epochs, learning_rate,
            batch_size, max_generate_items, use_lora, use_4bit
        ],
        outputs=training_config_state  # è¾“å‡ºåˆ°çŠ¶æ€ç»„ä»¶
    ).then(
        fn=start_training_interface,
        inputs=[training_config_state],  # ä»çŠ¶æ€ç»„ä»¶è¯»å–
        outputs=[train_status, status_display]
    )
    
    # æ£€æŸ¥MBPPæ•°æ®é›†
    def check_mbpp_dataset(mbpp_path):
        if not os.path.exists(mbpp_path):
            return f"âŒ MBPPæ•°æ®é›†ä¸å­˜åœ¨: {mbpp_path}"
        
        # è¯»å–æ ·æœ¬æ•°é‡
        try:
            count = 0
            with open(mbpp_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        count += 1
            
            # è¯»å–ç¤ºä¾‹
            with open(mbpp_path, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()
                example = first_line[:100] + "..." if len(first_line) > 100 else first_line
            
            return f"âœ… MBPPæ•°æ®é›†æ£€æŸ¥é€šè¿‡\næ ·æœ¬æ•°é‡: {count}\nç¤ºä¾‹: {example}"
        except Exception as e:
            return f"âŒ è¯»å–MBPPæ•°æ®é›†å¤±è´¥: {str(e)}"
    
    check_mbpp_btn.click(
        fn=check_mbpp_dataset,
        inputs=mbpp_dataset_path,
        outputs=check_mbpp_output
    )
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®é›†
    generate_btn.click(
        fn=generate_example_dataset,
        inputs=example_instructions,
        outputs=generate_output
    )
    
    # Tab 3: æ¨¡å‹è¯„ä¼°
    eval_config_state = gr.State({})

    def collect_eval_config(
        finetuned_path, human_eval_path_val,
        max_tasks_val, max_tokens_val, temperature_val, top_p_val
    ):
        # ä»æ¨¡å‹åŠ è½½é€‰é¡¹å¡è·å–æ¨¡å‹è·¯å¾„
        model_path_val = model_path.value if hasattr(model_path, 'value') else DEFAULT_CONFIG["model_path"]
        
        config = {
            "model_path": model_path_val,
            "finetuned_model_path": finetuned_path,
            "human_eval_path": human_eval_path_val,
            "max_tasks": int(max_tasks_val),
            "max_tokens": int(max_tokens_val),
            "temperature": float(temperature_val),
            "top_p": float(top_p_val)
        }
        return config
    
    eval_btn.click(
        fn=collect_eval_config,
        inputs=[
            finetuned_model_path, human_eval_path,
            max_tasks, max_tokens, temperature, top_p
        ],
        outputs=eval_config_state  # è¾“å‡ºåˆ°çŠ¶æ€ç»„ä»¶
    ).then(
        fn=start_evaluation_interface,
        inputs=[eval_config_state],  # ä»çŠ¶æ€ç»„ä»¶è¯»å–
        outputs=[eval_status, status_display]
    )
    
    # æŸ¥çœ‹ç»“æœ
    results_btn.click(
        fn=get_comparison_results,
        outputs=results_info
    )
    
    # æ£€æŸ¥è·¯å¾„
    check_btn.click(
        fn=check_paths,
        inputs=[model_path, finetuned_model_path, human_eval_path],
        outputs=check_output
    )
    
    # Tab 4: å¤§æ¨¡å‹é—®ç­”
    def start_qa_interface(instruction, temperature, top_p, max_new_tokens):
        """å¼€å§‹é—®ç­”ç•Œé¢å‡½æ•°"""
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åŠ è½½
        global model
        if model is None:
            return "âŒ æ¨¡å‹æœªåŠ è½½", "", "è¯·å…ˆåŠ è½½æ¨¡å‹", "æ¨¡å‹æœªåŠ è½½"
        
        if not instruction or instruction.strip() == "":
            return "âŒ è¯·è¾“å…¥æŒ‡ä»¤", "", "", "è¾“å…¥ä¸ºç©º"
        
        # ä½¿ç”¨é—®ç­”å‡½æ•°å¤„ç†æŒ‡ä»¤
        qa_status, code, save_status = process_instruction_with_local_model(
            instruction.strip(),
            temperature,
            top_p,
            max_new_tokens,
            mbpp_path=DEFAULT_CONFIG["mbpp_dataset_path"]
        )
        
        return qa_status, code, save_status, "å¤„ç†å®Œæˆ"
    
    # ç®€åŒ–äº‹ä»¶ç»‘å®šï¼Œç›´æ¥ä¼ é€’å‚æ•°
    start_qa_btn.click(
        fn=start_qa_interface,
        inputs=[instruction_input, gen_temperature, gen_top_p, max_new_tokens],
        outputs=[qa_status, code_output, save_status, status_display]
    )
    
    # ç¤ºä¾‹æŒ‡ä»¤æŒ‰é’®
    def set_example_instruction(example_text):
        return example_text
    
    example_instr1.click(
        fn=lambda: set_example_instruction("Write a function to add two numbers and return the sum"),
        outputs=instruction_input
    )
    
    example_instr2.click(
        fn=lambda: set_example_instruction("Write a function to check if a number is prime"),
        outputs=instruction_input
    )
    
    example_instr3.click(
        fn=lambda: set_example_instruction("Write a function to generate the first n Fibonacci numbers"),
        outputs=instruction_input
    )
    
    example_instr4.click(
        fn=lambda: set_example_instruction("è‡ªæˆ‘æ¼”åŒ–"),
        outputs=instruction_input
    )
    
    # ====== å®šæ—¶æ›´æ–° ======
    
    # æ›´æ–°ç³»ç»Ÿä¿¡æ¯
    demo.load(
        fn=update_system_info,
        outputs=[gpu_info, model_info],
        every=5
    )
    
    # æ›´æ–°æ—¥å¿—
    def update_all():
        logs = update_logs()
        
        # æ›´æ–°çŠ¶æ€
        global is_training, is_evaluating, is_generating
        status = "å‡†å¤‡å°±ç»ª"
        if is_training:
            status = "è®­ç»ƒä¸­..."
        elif is_evaluating:
            status = "è¯„ä¼°ä¸­..."
        elif is_generating:
            status = "ç”Ÿæˆä¸­..."
        
        # æ›´æ–°ç»“æœ
        results = "æš‚æ— ç»“æœ"
        global comparison_results
        if comparison_results:
            results = f"é€šè¿‡ç‡: {comparison_results.get('finetuned_pass_rate', 0):.1f}%"
        
        return logs, status, results
    
    demo.load(
        fn=update_all,
        outputs=[log_output, status_display, results_info],
        every=2
    )

# ====== ä¸»ç¨‹åº ======
if __name__ == "__main__":
    # æ£€æŸ¥ä¾èµ–
    required_packages = ["torch", "transformers", "gradio"]
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print("âŒ ç¼ºå°‘å¿…è¦çš„ä¾èµ–åº“:")
        for package in missing_packages:
            print(f"  - {package}")
        print("\nè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print("pip install torch transformers gradio")
        print("\nå¯é€‰å®‰è£…ï¼ˆç”¨äºå®Œæ•´åŠŸèƒ½ï¼‰:")
        print("pip install datasets accelerate peft requests")
        sys.exit(1)
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs("./models", exist_ok=True)
    os.makedirs(DEFAULT_CONFIG["output_dir"], exist_ok=True)
    os.makedirs("./datasets", exist_ok=True)
    os.makedirs("./mbpp_training_data", exist_ok=True)
    
    # æ£€æŸ¥MBPPæ•°æ®é›†
    mbpp_path = DEFAULT_CONFIG["mbpp_dataset_path"]
    if not os.path.exists(mbpp_path):
        print(f"âš ï¸ è­¦å‘Š: MBPPæ•°æ®é›†ä¸å­˜åœ¨: {mbpp_path}")
        print("å°†åˆ›å»ºæ–°çš„MBPPæ•°æ®é›†æ–‡ä»¶")
        with open(mbpp_path, 'w', encoding='utf-8') as f:
            f.write('"Write a function to add two numbers and return the sum"\n')
            f.write('"Write a function to check if a number is prime"\n')
            f.write('"Write a function to generate the first n Fibonacci numbers"\n')
        print(f"âœ… å·²åˆ›å»ºç¤ºä¾‹MBPPæ•°æ®é›†: {mbpp_path}")
    
    # æ£€æŸ¥HumanEvalæ•°æ®é›†
    if not os.path.exists(DEFAULT_CONFIG["human_eval_path"]):
        print(f"âš ï¸ è­¦å‘Š: HumanEvalæ•°æ®é›†ä¸å­˜åœ¨: {DEFAULT_CONFIG['human_eval_path']}")
        print("è¯·ä»ä»¥ä¸‹åœ°å€ä¸‹è½½:")
        print("https://github.com/openai/human-eval")
        print("ä¸‹è½½åä¿å­˜åˆ° ./datasets/ ç›®å½•")
    
    # å¯åŠ¨ç•Œé¢
    print("ğŸš€ å¯åŠ¨ Qwen2.5-Coder å®Œæ•´ç³»ç»Ÿ...")
    print(f"è®¿é—®åœ°å€: http://localhost:7860")
    print("æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
    
    try:
        demo.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=False,
            show_api=False
        )
    except KeyboardInterrupt:
        print("\nğŸ‘‹ æœåŠ¡å·²åœæ­¢")