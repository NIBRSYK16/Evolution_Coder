"""
Gradio UIç•Œé¢æ¨¡å—
"""
import os
import json
import gradio as gr
from datetime import datetime
from ..config.settings import DEFAULT_CONFIG
from ..utils import log, log_collector
from ..models import load_model_interface
from ..training import start_training_interface, generate_mbpp_training_data
from ..evaluation import start_evaluation_interface, get_comparison_results
from ..utils import process_instruction_with_local_model

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    # ====== åˆ›å»ºå®Œæ•´çš„Gradioç•Œé¢ ======
    with gr.Blocks(title="Qwen2.5-Coder å®Œæ•´ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("# ğŸ¤– Qwen2.5-Coder å®Œæ•´ç³»ç»Ÿ")
        gr.Markdown("æ¨¡å‹åŠ è½½ã€å¾®è°ƒã€è¯„ä¼°ä¸€ä½“åŒ–ç³»ç»Ÿ")
        
        # æ·»åŠ ä¸€ä¸ªéšè—çš„çŠ¶æ€ç»„ä»¶æ¥å­˜å‚¨è®­ç»ƒé…ç½®
        training_config_state = gr.State({})

        # çŠ¶æ€æ˜¾ç¤º
        with gr.Row():
            status_display = gr.Textbox(
                label="ç³»ç»ŸçŠ¶æ€",
                value="å‡†å¤‡å°±ç»ª",
                interactive=False,
                lines=1,
                scale=3
            )
            
            clear_logs_btn = gr.Button("æ¸…ç©ºæ—¥å¿—", variant="secondary", size="sm", scale=1)
        
        # æ—¥å¿—è¾“å‡ºåŒºåŸŸ
        log_output = gr.Textbox(
            label="ç³»ç»Ÿæ—¥å¿—",
            value="æ¬¢è¿ä½¿ç”¨ Qwen2.5-Coder å®Œæ•´ç³»ç»Ÿ\n\nè¯·é€‰æ‹©é€‰é¡¹å¡å¼€å§‹æ“ä½œã€‚",
            lines=25,
            interactive=False
        )
        
        # ç³»ç»Ÿä¿¡æ¯
        with gr.Row():
            gpu_info = gr.Textbox(
                label="GPUçŠ¶æ€",
                value="æ­£åœ¨æ£€æµ‹...",
                lines=2,
                interactive=False,
                scale=1
            )
            
            model_info = gr.Textbox(
                label="æ¨¡å‹ä¿¡æ¯",
                value="æœªåŠ è½½",
                lines=2,
                interactive=False,
                scale=1
            )
            
            results_info = gr.Textbox(
                label="æœ€æ–°ç»“æœ",
                value="æš‚æ— ç»“æœ",
                lines=2,
                interactive=False,
                scale=1
            )
        
        # é€‰é¡¹å¡
        with gr.Tabs():
            # ====== Tab 1: æ¨¡å‹åŠ è½½ ======
            with gr.TabItem("ğŸš€ æ¨¡å‹åŠ è½½"):
                with gr.Row():
                    with gr.Column(scale=1):
                        model_path = gr.Textbox(
                            label="æ¨¡å‹è·¯å¾„",
                            value=DEFAULT_CONFIG["model_path"],
                            placeholder="è¾“å…¥æ¨¡å‹è·¯å¾„"
                        )
                        load_btn = gr.Button("åŠ è½½æ¨¡å‹", variant="primary")
                        load_status = gr.Textbox(label="åŠ è½½çŠ¶æ€", interactive=False)
                    
                    with gr.Column(scale=2):
                        gr.Markdown("""
### æ¨¡å‹åŠ è½½è¯´æ˜

1. **æ¨¡å‹è·¯å¾„**: è¾“å…¥Qwen2.5-Coderæ¨¡å‹çš„æœ¬åœ°è·¯å¾„
2. **ç‚¹å‡»åŠ è½½**: ç³»ç»Ÿä¼šè‡ªåŠ¨åŠ è½½æ¨¡å‹å’Œtokenizer
3. **åŠ è½½æ—¶é—´**: é¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ

### é»˜è®¤æ¨¡å‹ä¿¡æ¯
- æ¨¡å‹: Qwen2.5-Coder-0.5B-Instruct
- å¤§å°: çº¦500MB
- æ”¯æŒ: ä»£ç ç”Ÿæˆã€é—®é¢˜æ±‚è§£
- è®¾å¤‡: è‡ªåŠ¨é€‰æ‹©CUDA(GPU)æˆ–CPU
                        """)
            
            # ====== Tab 2: æ¨¡å‹å¾®è°ƒï¼ˆå·²ä¿®æ”¹ä¸ºä½¿ç”¨MBPPï¼‰ ======
            with gr.TabItem("ğŸ¯ æ¨¡å‹å¾®è°ƒ"):
                with gr.Row():
                    with gr.Column(scale=1):
                        mbpp_dataset_path = gr.Textbox(
                            label="MBPPæ•°æ®é›†è·¯å¾„",
                            value=DEFAULT_CONFIG["mbpp_dataset_path"],
                            placeholder="MBPPæ•°æ®é›†è·¯å¾„"
                        )
                        output_dir = gr.Textbox(
                            label="è¾“å‡ºç›®å½•",
                            value=DEFAULT_CONFIG["output_dir"],
                            placeholder="å¾®è°ƒæ¨¡å‹è¾“å‡ºç›®å½•"
                        )
                        
                        gr.Markdown("### è®­ç»ƒå‚æ•°")
                        num_epochs = gr.Number(
                            label="è®­ç»ƒè½®æ•°",
                            value=DEFAULT_CONFIG["num_epochs"],
                            precision=0
                        )
                        learning_rate = gr.Number(
                            label="å­¦ä¹ ç‡",
                            value=DEFAULT_CONFIG["learning_rate"]
                        )
                        batch_size = gr.Number(
                            label="æ‰¹å¤§å°",
                            value=DEFAULT_CONFIG["batch_size"],
                            precision=0
                        )
                        
                        gr.Markdown("### æ•°æ®ç”Ÿæˆå‚æ•°")
                        max_generate_items = gr.Number(
                            label="æœ€å¤§ç”Ÿæˆæ•°é‡",
                            value=DEFAULT_CONFIG["max_generate_items"],
                            precision=0
                        )
                        use_lora = gr.Checkbox(label="ä½¿ç”¨LoRA", value=DEFAULT_CONFIG["use_lora"])
                        use_4bit = gr.Checkbox(label="ä½¿ç”¨4bité‡åŒ–", value=DEFAULT_CONFIG["use_4bit"])
                        
                        train_btn = gr.Button("å¼€å§‹å¾®è°ƒ", variant="primary")
                        train_status = gr.Textbox(label="å¾®è°ƒçŠ¶æ€", interactive=False, lines=3)
                    
                    with gr.Column(scale=1):
                        gr.Markdown("""
### å¾®è°ƒè¯´æ˜

1. **MBPPæ•°æ®é›†**: åŒ…å«ç¼–ç¨‹ä»»åŠ¡çš„æ•°æ®é›†
2. **è®­ç»ƒå‚æ•°**: è°ƒæ•´å­¦ä¹ ç‡å’Œæ‰¹å¤§å°ä»¥ä¼˜åŒ–æ€§èƒ½
3. **LoRAå¾®è°ƒ**: ä½¿ç”¨LoRAæŠ€æœ¯å‡å°‘æ˜¾å­˜å ç”¨
4. **æ•°æ®ç”Ÿæˆ**: è‡ªåŠ¨ä»MBPPç”Ÿæˆè®­ç»ƒæ•°æ®å¯¹

### æ“ä½œæ­¥éª¤

1. ç¡®ä¿MBPPæ•°æ®é›†å­˜åœ¨
2. é…ç½®è®­ç»ƒå‚æ•°
3. ç‚¹å‡»"å¼€å§‹å¾®è°ƒ"å¼€å§‹è®­ç»ƒ
4. å¾®è°ƒå®Œæˆåæ¨¡å‹ä¿å­˜åœ¨è¾“å‡ºç›®å½•

### æ³¨æ„äº‹é¡¹

- é¦–æ¬¡å¾®è°ƒéœ€è¦ç”Ÿæˆè®­ç»ƒæ•°æ®
- ç”Ÿæˆè¿‡ç¨‹éœ€è¦è°ƒç”¨API
- å»ºè®®ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒ
                        """)
                        
                        gr.Markdown("### æ•°æ®é›†æ£€æŸ¥")
                        check_mbpp_btn = gr.Button("æ£€æŸ¥MBPPæ•°æ®é›†")
                        check_mbpp_output = gr.Textbox(label="æ£€æŸ¥ç»“æœ", interactive=False, lines=3)
                        
                        gr.Markdown("### ç”Ÿæˆç¤ºä¾‹æ•°æ®é›†")
                        example_instructions = gr.Textbox(
                            label="ç¤ºä¾‹æŒ‡ä»¤",
                            value="Write a function to add two numbers\nWrite a function to check if a number is prime",
                            placeholder="æ¯è¡Œä¸€æ¡æŒ‡ä»¤",
                            lines=3
                        )
                        generate_btn = gr.Button("ç”Ÿæˆæ•°æ®é›†")
                        generate_output = gr.Textbox(label="ç”Ÿæˆç»“æœ", interactive=False, lines=2)
            
            # ====== Tab 3: æ¨¡å‹è¯„ä¼° ======
            with gr.TabItem("ğŸ“Š æ¨¡å‹è¯„ä¼°"):
                with gr.Row():
                    with gr.Column(scale=1):
                        finetuned_model_path = gr.Textbox(
                            label="å¾®è°ƒæ¨¡å‹è·¯å¾„",
                            value=DEFAULT_CONFIG["finetuned_model_path"],
                            placeholder="å¾®è°ƒåæ¨¡å‹è·¯å¾„"
                        )
                        human_eval_path = gr.Textbox(
                            label="HumanEvalæ•°æ®é›†è·¯å¾„",
                            value=DEFAULT_CONFIG["human_eval_path"],
                            placeholder="HumanEvalæ•°æ®é›†è·¯å¾„"
                        )
                        
                        gr.Markdown("### è¯„ä¼°å‚æ•°")
                        max_tasks = gr.Number(
                            label="æœ€å¤§è¯„ä¼°ä»»åŠ¡æ•°",
                            value=DEFAULT_CONFIG["max_tasks"],
                            precision=0
                        )
                        max_tokens = gr.Number(
                            label="æœ€å¤§ç”Ÿæˆtokenæ•°",
                            value=DEFAULT_CONFIG["max_tokens"],
                            precision=0
                        )
                        temperature = gr.Slider(
                            label="æ¸©åº¦",
                            value=DEFAULT_CONFIG["temperature"],
                            minimum=0,
                            maximum=1,
                            step=0.1
                        )
                        top_p = gr.Slider(
                            label="Top P",
                            value=DEFAULT_CONFIG["top_p"],
                            minimum=0,
                            maximum=1,
                            step=0.1
                        )
                        
                        eval_btn = gr.Button("å¼€å§‹è¯„ä¼°", variant="primary")
                        eval_status = gr.Textbox(label="è¯„ä¼°çŠ¶æ€", interactive=False, lines=3)
                    
                    with gr.Column(scale=1):
                        gr.Markdown("""
### è¯„ä¼°è¯´æ˜

1. **æ¨¡å‹å¯¹æ¯”**: è‡ªåŠ¨å¯¹æ¯”åŸå§‹å’Œå¾®è°ƒæ¨¡å‹
2. **HumanEvalæ•°æ®é›†**: æ ‡å‡†ä»£ç ç”Ÿæˆè¯„ä¼°é›†
3. **è¯„ä¼°æŒ‡æ ‡**: é€šè¿‡ç‡ã€è€—æ—¶ç­‰

### æ“ä½œæ­¥éª¤

1. ç¡®ä¿ä¸¤ä¸ªæ¨¡å‹è·¯å¾„éƒ½å­˜åœ¨
2. HumanEvalæ•°æ®é›†å¯ä»GitHubè·å–
3. ç‚¹å‡»"å¼€å§‹è¯„ä¼°"å¼€å§‹è¯„ä¼°è¿‡ç¨‹
4. ç»“æœä¼šè‡ªåŠ¨ä¿å­˜ä¸ºJSONæ–‡ä»¶

### è·å–HumanEvalæ•°æ®é›†

```bash
git clone https://github.com/openai/human-eval.git
```

å°†æ•°æ®é›†å¤åˆ¶åˆ° `./datasets/` ç›®å½•

### ç»“æœæŸ¥çœ‹

è¯„ä¼°å®Œæˆåç‚¹å‡»"æŸ¥çœ‹ç»“æœ"æŸ¥çœ‹è¯¦ç»†çš„å¯¹æ¯”ä¿¡æ¯
                        """)
                        
                        gr.Markdown("### è·¯å¾„æ£€æŸ¥")
                        check_btn = gr.Button("æ£€æŸ¥è·¯å¾„")
                        check_output = gr.Textbox(label="æ£€æŸ¥ç»“æœ", interactive=False, lines=4)
                        
                        gr.Markdown("### æŸ¥çœ‹è¯„ä¼°ç»“æœ")
                        results_btn = gr.Button("æŸ¥çœ‹ç»“æœ")
            
            # ====== Tab 4: å¤§æ¨¡å‹é—®ç­”ï¼ˆæ–°å¢ï¼‰ ======
            with gr.TabItem("ğŸ’¬ å¤§æ¨¡å‹é—®ç­”"):
                with gr.Row():
                    with gr.Column(scale=1):
                        instruction_input = gr.Textbox(
                            label="è¾“å…¥æŒ‡ä»¤æˆ–ä»£ç è¦æ±‚",
                            placeholder="è¾“å…¥æ‚¨çš„ä»£ç éœ€æ±‚...",
                            lines=3
                        )
                        
                        gr.Markdown("### ç”Ÿæˆå‚æ•°")
                        gen_temperature = gr.Slider(
                            label="æ¸©åº¦",
                            value=DEFAULT_CONFIG["gen_temperature"],
                            minimum=0,
                            maximum=1,
                            step=0.1
                        )
                        gen_top_p = gr.Slider(
                            label="Top P",
                            value=DEFAULT_CONFIG["gen_top_p"],
                            minimum=0,
                            maximum=1,
                            step=0.1
                        )
                        max_new_tokens = gr.Number(
                            label="æœ€å¤§tokenæ•°",
                            value=DEFAULT_CONFIG["max_new_tokens"],
                            precision=0
                        )
                        
                        start_qa_btn = gr.Button("ç”Ÿæˆä»£ç ", variant="primary")
                        qa_status = gr.Textbox(label="ç”ŸæˆçŠ¶æ€", interactive=False)
                        save_status = gr.Textbox(label="ä¿å­˜çŠ¶æ€", interactive=False)
                    
                    with gr.Column(scale=2):
                        code_output = gr.Textbox(
                            label="ç”Ÿæˆçš„ä»£ç ",
                            interactive=False,
                            lines=15
                        )
                
                with gr.Row():
                    gr.Markdown("""
### å¿«é€Ÿç¤ºä¾‹

ç‚¹å‡»ä¸‹é¢çš„æŒ‰é’®å¿«é€ŸåŠ è½½ç¤ºä¾‹æŒ‡ä»¤
                    """)
                
                with gr.Row():
                    example_instr1 = gr.Button("ç¤ºä¾‹1: åŠ æ³•å‡½æ•°")
                    example_instr2 = gr.Button("ç¤ºä¾‹2: è´¨æ•°æ£€æµ‹")
                    example_instr3 = gr.Button("ç¤ºä¾‹3: æ–æ³¢é‚£å¥‘æ•°åˆ—")
                    example_instr4 = gr.Button("ç¤ºä¾‹4: è‡ªæˆ‘æ¼”åŒ–")
        
        # ====== äº‹ä»¶ç»‘å®š ======
        
        # æ¸…ç©ºæ—¥å¿—
        def clear_logs():
            log_collector.clear()
            return "æ—¥å¿—å·²æ¸…ç©º"
        
        clear_logs_btn.click(
            fn=clear_logs,
            outputs=log_output
        )
        
        # Tab 1: æ¨¡å‹åŠ è½½
        load_btn.click(
            fn=load_model_interface,
            inputs=[model_path],
            outputs=[load_status, status_display]
        ).then(
            fn=update_system_info,
            outputs=[gpu_info, model_info]
        )
        
        # Tab 2: æ¨¡å‹å¾®è°ƒ
        def collect_training_config(
            model_path_val,
            mbpp_dataset_path_val, output_dir_val, 
            num_epochs_val, learning_rate_val,
            batch_size_val, max_generate_items_val, use_lora_val, use_4bit_val
        ):
            # ç¡®ä¿æ¨¡å‹è·¯å¾„ä¸ä¸ºç©º
            if not model_path_val:
                model_path_val = DEFAULT_CONFIG["model_path"]
            
            config = {
                "model_path": model_path_val,
                "mbpp_dataset_path": mbpp_dataset_path_val,
                "output_dir": output_dir_val,
                "num_epochs": int(num_epochs_val),
                "learning_rate": float(learning_rate_val),
                "batch_size": int(batch_size_val),
                "max_generate_items": int(max_generate_items_val),
                "use_lora": use_lora_val,
                "use_4bit": use_4bit_val
            }
            return config
        
        train_btn.click(
            fn=collect_training_config,
            inputs=[
                model_path,
                mbpp_dataset_path, output_dir,
                num_epochs, learning_rate,
                batch_size, max_generate_items, use_lora, use_4bit
            ],
            outputs=training_config_state  # è¾“å‡ºåˆ°çŠ¶æ€ç»„ä»¶
        ).then(
            fn=start_training_interface,
            inputs=[training_config_state],  # ä»çŠ¶æ€ç»„ä»¶è¯»å–
            outputs=[train_status, status_display]
        )
        
        # æ£€æŸ¥MBPPæ•°æ®é›†
        def check_mbpp_dataset(mbpp_path):
            if not os.path.exists(mbpp_path):
                return f"âŒ MBPPæ•°æ®é›†ä¸å­˜åœ¨: {mbpp_path}"
            
            # è¯»å–æ ·æœ¬æ•°é‡
            try:
                count = 0
                with open(mbpp_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            count += 1
                
                # è¯»å–ç¤ºä¾‹
                with open(mbpp_path, 'r', encoding='utf-8') as f:
                    first_line = f.readline().strip()
                    example = first_line[:100] + "..." if len(first_line) > 100 else first_line
                
                return f"âœ… MBPPæ•°æ®é›†æ£€æŸ¥é€šè¿‡\næ ·æœ¬æ•°é‡: {count}\nç¤ºä¾‹: {example}"
            except Exception as e:
                return f"âŒ è¯»å–MBPPæ•°æ®é›†å¤±è´¥: {str(e)}"
        
        check_mbpp_btn.click(
            fn=check_mbpp_dataset,
            inputs=mbpp_dataset_path,
            outputs=check_mbpp_output
        )
        
        # ç”Ÿæˆç¤ºä¾‹æ•°æ®é›†
        def generate_example_dataset(instructions):
            lines = instructions.strip().split('\n')
            dataset = []
            
            for i, instr in enumerate(lines):
                if instr.strip():
                    # ç®€å•ç”Ÿæˆå¯¹åº”çš„ä»£ç 
                    if "add" in instr.lower() and "number" in instr.lower():
                        code = """def add_numbers(a, b):
    \"\"\"Add two numbers\"\"\"
    return a + b"""
                    elif "prime" in instr.lower():
                        code = """def is_prime(n):
    \"\"\"Check if a number is prime\"\"\"
    if n <= 1:
        return False
    for i in range(2, int(n**0.5) + 1):
        if n % i == 0:
            return False
    return True"""
                    else:
                        code = f"""def function_{i}():
    \"\"\"Generated function\"\"\"
    # TODO: Implement this function
    pass"""
                    
                    dataset.append({
                        "instruction": instr.strip(),
                        "code": code
                    })
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            output_path = "./example_dataset.jsonl"
            with open(output_path, 'w', encoding='utf-8') as f:
                for item in dataset:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')
            
            return f"âœ… ç¤ºä¾‹æ•°æ®é›†å·²ç”Ÿæˆ: {output_path}\nå…± {len(dataset)} ä¸ªæ ·æœ¬"
        
        generate_btn.click(
            fn=generate_example_dataset,
            inputs=example_instructions,
            outputs=generate_output
        )
        
        # Tab 3: æ¨¡å‹è¯„ä¼°
        eval_config_state = gr.State({})

        def collect_eval_config(
            model_path_val,
            finetuned_path, human_eval_path_val,
            max_tasks_val, max_tokens_val, temperature_val, top_p_val
        ):
            # ç¡®ä¿æ¨¡å‹è·¯å¾„ä¸ä¸ºç©º
            if not model_path_val:
                model_path_val = DEFAULT_CONFIG["model_path"]
            
            config = {
                "model_path": model_path_val,
                "finetuned_model_path": finetuned_path,
                "human_eval_path": human_eval_path_val,
                "max_tasks": int(max_tasks_val),
                "max_tokens": int(max_tokens_val),
                "temperature": float(temperature_val),
                "top_p": float(top_p_val)
            }
            return config
        
        eval_btn.click(
            fn=collect_eval_config,
            inputs=[
                model_path,
                finetuned_model_path, human_eval_path,
                max_tasks, max_tokens, temperature, top_p
            ],
            outputs=eval_config_state  # è¾“å‡ºåˆ°çŠ¶æ€ç»„ä»¶
        ).then(
            fn=start_evaluation_interface,
            inputs=[eval_config_state],  # ä»çŠ¶æ€ç»„ä»¶è¯»å–
            outputs=[eval_status, status_display]
        )
        
        # æŸ¥çœ‹ç»“æœ
        results_btn.click(
            fn=get_comparison_results,
            outputs=results_info
        )
        
        # æ£€æŸ¥è·¯å¾„
        def check_paths(model_path_val, finetuned_model_path_val, human_eval_path_val):
            """æ£€æŸ¥è·¯å¾„"""
            results = []
            
            # æ£€æŸ¥åŸå§‹æ¨¡å‹
            if os.path.exists(model_path_val):
                results.append(f"âœ… åŸå§‹æ¨¡å‹è·¯å¾„å­˜åœ¨: {model_path_val}")
            else:
                results.append(f"âŒ åŸå§‹æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path_val}")
            
            # æ£€æŸ¥å¾®è°ƒæ¨¡å‹
            if os.path.exists(finetuned_model_path_val):
                results.append(f"âœ… å¾®è°ƒæ¨¡å‹è·¯å¾„å­˜åœ¨: {finetuned_model_path_val}")
            else:
                results.append(f"âŒ å¾®è°ƒæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {finetuned_model_path_val}")
            
            # æ£€æŸ¥æ•°æ®é›†
            if os.path.exists(human_eval_path_val):
                results.append(f"âœ… HumanEvalæ•°æ®é›†å­˜åœ¨: {human_eval_path_val}")
            else:
                results.append(f"âŒ HumanEvalæ•°æ®é›†ä¸å­˜åœ¨: {human_eval_path_val}")
                results.append("è¯·ä» https://github.com/openai/human-eval ä¸‹è½½æ•°æ®é›†")
            
            return "\n".join(results)
        
        check_btn.click(
            fn=check_paths,
            inputs=[model_path, finetuned_model_path, human_eval_path],
            outputs=check_output
        )
        
        # Tab 4: å¤§æ¨¡å‹é—®ç­”
        def start_qa_interface(instruction, temperature, top_p, max_new_tokens):
            """å¼€å§‹é—®ç­”ç•Œé¢å‡½æ•°"""
            from ..models import is_model_loaded
            
            if not is_model_loaded():
                return "âŒ æ¨¡å‹æœªåŠ è½½", "", "è¯·å…ˆåŠ è½½æ¨¡å‹", "æ¨¡å‹æœªåŠ è½½"
            
            if not instruction or instruction.strip() == "":
                return "âŒ è¯·è¾“å…¥æŒ‡ä»¤", "", "", "è¾“å…¥ä¸ºç©º"
            
            # ä½¿ç”¨é—®ç­”å‡½æ•°å¤„ç†æŒ‡ä»¤
            qa_status, code, save_status = process_instruction_with_local_model(
                instruction.strip(),
                temperature,
                top_p,
                max_new_tokens,
                mbpp_path=DEFAULT_CONFIG["mbpp_dataset_path"]
            )
            
            return qa_status, code, save_status, "å¤„ç†å®Œæˆ"
        
        # ç®€åŒ–äº‹ä»¶ç»‘å®šï¼Œç›´æ¥ä¼ é€’å‚æ•°
        start_qa_btn.click(
            fn=start_qa_interface,
            inputs=[instruction_input, gen_temperature, gen_top_p, max_new_tokens],
            outputs=[qa_status, code_output, save_status, status_display]
        )
        
        # ç¤ºä¾‹æŒ‡ä»¤æŒ‰é’®
        def set_example_instruction(example_text):
            return example_text
        
        example_instr1.click(
            fn=lambda: set_example_instruction("Write a function to add two numbers and return the sum"),
            outputs=instruction_input
        )
        
        example_instr2.click(
            fn=lambda: set_example_instruction("Write a function to check if a number is prime"),
            outputs=instruction_input
        )
        
        example_instr3.click(
            fn=lambda: set_example_instruction("Write a function to generate the first n Fibonacci numbers"),
            outputs=instruction_input
        )
        
        example_instr4.click(
            fn=lambda: set_example_instruction("è‡ªæˆ‘æ¼”åŒ–"),
            outputs=instruction_input
        )
        
        # ====== å®šæ—¶æ›´æ–° ======
        
        # æ›´æ–°ç³»ç»Ÿä¿¡æ¯
        demo.load(
            fn=update_system_info,
            outputs=[gpu_info, model_info],
            every=5
        )
        
        # æ›´æ–°æ—¥å¿—
        def update_all():
            import torch
            logs = log_collector.get_logs()
            
            # æ›´æ–°çŠ¶æ€
            status = "å‡†å¤‡å°±ç»ª"
            
            # æ›´æ–°ç»“æœ
            results = "æš‚æ— ç»“æœ"
            
            return logs, status, results
        
        demo.load(
            fn=update_all,
            outputs=[log_output, status_display, results_info],
            every=2
        )
    
    return demo

def update_system_info():
    """æ›´æ–°ç³»ç»Ÿä¿¡æ¯"""
    import torch
    from ..models import get_model
    
    gpu_text = ""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        gpu_text = f"âœ… GPUå¯ç”¨\nåç§°: {gpu_name}\næ˜¾å­˜: {gpu_memory:.1f} GB"
    else:
        gpu_text = "âŒ æœªæ£€æµ‹åˆ°GPU\nå°†åœ¨CPUä¸Šè¿è¡Œï¼Œé€Ÿåº¦è¾ƒæ…¢"
    
    model_text = "âŒ æ¨¡å‹æœªåŠ è½½"
    model, _, _ = get_model()
    if model is not None:
        model_text = "âœ… æ¨¡å‹å·²åŠ è½½\nå¯ä½¿ç”¨ç”Ÿæˆå’Œå¾®è°ƒåŠŸèƒ½"
    
    return gpu_text, model_text
